### 1 Classification

1. Derive the dual formulation of the soft support vector machine algorithm.
#### Solution of T1
软间隔SVM原始问题:  
**最小化：**  
$$\frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i$$  
**满足约束：**  
$$y_i(w \cdot x_i + b) \geq 1 - \xi_i, \xi_i \geq 0$$  
设$\alpha_i \geq 0$,$\mu_i \geq 0$，构造拉格朗日函数：
$$
L(w, b, \xi, \alpha, \mu) = \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n \alpha_i [y_i(w^T x_i + b) - 1 + \xi_i] - \sum_{i=1}^n \mu_i \xi_i
$$
对 $w$、$b$、$\xi_i$ 求偏导:
$$
\frac{\partial L}{\partial w} = w - \sum_{i=1}^n \alpha_i y_i x_i = 0 \Rightarrow w = \sum_{i=1}^n \alpha_i y_i x_i
$$
$$
\frac{\partial L}{\partial b} = -\sum_{i=1}^n \alpha_i y_i = 0 \Rightarrow \sum_{i=1}^n \alpha_i y_i = 0
$$
$$
\frac{\partial L}{\partial \xi_i} = C - \alpha_i - \mu_i = 0 \Rightarrow \alpha_i = C - \mu_i
$$
代入拉格朗日对偶函数,
$$
\begin{aligned}
\mathcal{G}(\alpha,\mu) =& \min\limits_{w,b,\xi}L(w, b, \xi, \alpha, \mu) \\
=&\frac{1}{2} \|\sum_{i=1}^n \alpha_i y_i x_i \|^2 + C \sum_{i=1}^n \xi_i \\ 
&- \sum_{i=1}^n \alpha_i [y_i(\sum_{j=1}^n \alpha_j y_j x_j^T x_i + b) - 1 + \xi_i] - \sum_{i=1}^n \mu_i \xi_i \\
=& \frac{1}{2} \left(\sum_{i=1}^n \alpha_i y_i x_i\right)^T \cdot \left(\sum_{j=1}^n \alpha_j y_j x_j\right) \\
&- \sum_{i=1}^n \alpha_i [y_i(\sum_{j=1}^n \alpha_j y_j x_j^T x_i + b) - 1]\\
=& \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j - \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j \\
& - b\sum_{i=1}^n\alpha_i y_i+ \sum_{i=1}^n \alpha_i\\
=& \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j
\end{aligned}
$$
从而得到对偶问题：
$$
\max_{\alpha,\mu}\mathcal{G}(\alpha,\mu) = \max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j
$$
约束条件为：
$$
\sum_{i=1}^n \alpha_i y_i = 0, \quad 0 \leq \alpha_i \leq C, \quad i = 1, \dots, n
$$

---
2. Derive the Discriminant function of the Quadratic Discriminant Analysis.
#### Solution of T2

QDA假设对于每一个类别k，其特征向量x服从多元高斯分布：
$$
p(x | Y = k) = \frac{1}{(2\pi)^{p/2} |\Sigma_k|^{1/2}} \exp\left(-\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)\right)
$$
记$\pi_k = P(Y=k)$，由Bayes，后验概率为：
$$
P(Y = k | x) = \frac{\pi_k p(x | Y = k)}{\sum_{l=1}^K \pi_l p(x | Y = l)}
$$
决策规则为:
$$
\begin{aligned}
\hat{Y} =& \arg\max_k P(Y = k | x) \\
=&\arg\max_k \frac{\pi_k p(x | Y = k)}{\sum_{l=1}^K \pi_l p(x | Y = l)}
\end{aligned}
$$

取对数并忽略常数项：
$$
\begin{aligned}
\hat{Y} = \arg\max_k -\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k) - \frac{1}{2} \log |\Sigma_k| + \log \pi_k 
\end{aligned}
$$
从而导出了QDA的判别函数
$$
\delta_k(\mathbf{x}) = -\frac{1}{2} (\mathbf{x} - \mu_k)^\top \Sigma_k^{-1} (\mathbf{x} - \mu_k) - \frac{1}{2} \log |\Sigma_k| + \log \pi_k
$$

---
3. Derive the Gaussian process classifier by using the Laplace approximation.

#### Solution of T3
对分类问题而言，观测目标 $y_i \in \{0, 1\}$ 离散，高斯噪声模型 $y_i = f(x_i) + \epsilon_i$ 不再适用。因此引入潜函数 $f(\mathbf{x})$ ，并利用链接函数(通常为sigmoid函数)将其映射到概率。

为潜函数赋予一个高斯过程先验:
$$
p(\mathbf{f} | X) = \mathcal{N}(\mathbf{f} | \mathbf{0}, K)
$$
其中 $K_{ij} = k(\mathbf{x}_i, \mathbf{x}_j)$ 。

给定潜函数 $\mathbf{f}$，$\mathbf{y}$是伯努利随机变量构成的随机向量
$$
p(\mathbf{y} | \mathbf{f}) = \prod_{i=1}^n p(y_i | f_i) = \prod_{i=1}^n \sigma(f_i)^{y_i} (1 - \sigma(f_i))^{1 - y_i}
$$
要求潜函数的后验分布 $p(\mathbf{f} | X, \mathbf{y})$ ,根据Bayes: $p(\mathbf{f} | X, \mathbf{y}) = \frac{p(\mathbf{y} | \mathbf{f})  p(\mathbf{f} | X)}{p(\mathbf{y} | X)}$
分母为正则化项。由于 $p(\mathbf{y} | \mathbf{f})$ 非高斯，因此考虑拉普拉斯近似。以下求解近似分布 $q(\mathbf{f} | X, \mathbf{y})$ 

先求后验分布的众数 $\hat{\mathbf{f}}$。对后验分布取对数，得到：
$$
\hat{\mathbf{f}} = \arg\max_{\mathbf{f}} [\log p(\mathbf{y} | \mathbf{f}) + \log p(\mathbf{f} | X)]
$$

定义 $\Psi(\mathbf{f}) \triangleq \log p(\mathbf{y} | \mathbf{f}) + \log p(\mathbf{f} | X)$
其中 $$\log p(\mathbf{f} | X) = -\frac{1}{2} \mathbf{f}^\top K^{-1} \mathbf{f} - \frac{1}{2} \log |K| - \frac{n}{2} \log 2\pi$$ 
$$
\log p(\mathbf{y} | \mathbf{f}) = \sum_{i=1}^n [y_i \log \sigma(f_i) + (1-y_i) \log (1 - \sigma(f_i))]$$
所以，
$$
\Psi(\mathbf{f}) = \log p(\mathbf{y} | \mathbf{f}) - \frac{1}{2} \mathbf{f}^\top K^{-1} \mathbf{f} + \text{const.}
$$
令 $\nabla \Psi(\mathbf{f}) = \mathbf{0}$，$\nabla \Psi(\mathbf{f}) = \nabla \log p(\mathbf{y} | \mathbf{f}) - K^{-1} \mathbf{f}$
其中， $\nabla \log p(\mathbf{y} | \mathbf{f})$  的第i个分量为：

$$
\frac{\partial}{\partial f_i} \log p(\mathbf{y} | \mathbf{f}) = y_i - \sigma(f_i)
$$

因此，

$$
\nabla \Psi(\mathbf{f}) = (\mathbf{y} - \boldsymbol{\sigma}) - K^{-1} \mathbf{f}
$$

其中 $\boldsymbol{\sigma} = [\sigma(f_1), \dots, \sigma(f_n)]^T$。从而可以反解出众数 $\hat{\mathbf{f}}$。

以下计算众数处的Hessian矩阵以进行Laplace近似。

$$
\nabla \nabla \Psi(\mathbf{f}) = \nabla \nabla \log p(\mathbf{y} | \mathbf{f}) - K^{-1}
$$

其中 $\nabla \nabla \log p(\mathbf{y} | \mathbf{f})$ 是一个对角矩阵，其对角线元素为:
$$
\frac{\partial^2}{\partial f_i^2} \log p(y_i | f_i) = \frac{\partial}{\partial f_i} (y_i - \sigma(f_i)) = -\sigma(f_i)(1 - \sigma(f_i))
$$

所以，在众数 $\hat{\mathbf{f}}$ 处，Hessian矩阵为：
$$
H = \nabla \nabla \Psi(\hat{\mathbf{f}}) = -\hat{W} - K^{-1}
$$

其中 $\hat{W}$ 是对角阵，$\hat{W}_{ii} = \sigma(\hat{f}_i)(1 - \sigma(\hat{f}_i))$。从而近似分布 $q(\mathbf{f} | X, \mathbf{y}) = \mathcal{N}(\mathbf{f} | \hat{\mathbf{f}}, \Sigma)$，其中协方差矩阵 $ \Sigma = (-H)^{-1} = (\hat{W} + K^{-1})^{-1} $。
对于预测点 $\mathbf{x}_*$ ，下求其分类标签 $y_*$ 。

由高斯过程的性质，$\mathbf{f}$ 和$f_*$ 的联合先验是高斯分布：
$$
\begin{bmatrix} \mathbf{f} \\ f_* \end{bmatrix} \sim \mathcal{N}\left( \mathbf{0}, \begin{bmatrix} K & \mathbf{k}_* \\ \mathbf{k}_*^\top & k_{**} \end{bmatrix} \right)
$$
其中 $\mathbf{k}_* = [k(\mathbf{x}_1, \mathbf{x}_*), \dots, k(\mathbf{x}_n, \mathbf{x}_*)]^\top $，$ k_{**} = k(\mathbf{x}_*, \mathbf{x}_*)$。

则 $f_*$ 的预测分布为：
$$
q(f_* | X, \mathbf{y}, \mathbf{x}_*) = \int p(f_* | X, \mathbf{x}_*, \mathbf{f})  q(\mathbf{f} | X, \mathbf{y})  d\mathbf{f} = \mathcal{N}(f_* | \mu_*, \sigma^2_*)
$$

其中 $\mu_* = \mathbf{k}_*^\top K^{-1} \hat{\mathbf{f}} , \sigma^2_* = k_{**} - \mathbf{k}_*^\top (K + \hat{W}^{-1})^{-1} \mathbf{k}_*$

预测概率如下(各项参数见上方表达式): $$P(y_*=1 | X, \mathbf{y}, \mathbf{x}_*) = \int \sigma(f_*)  q(f_* | X, \mathbf{y}, \mathbf{x}_*)  df_*$$

---
4. For the following data, build a tree by using: a) Gini index, b) the depth of tree is two. The target variable is **Loan Approved**.


#### Solution of T4

首先计算每个特征的Gini

##### **1. Age**
取中位数36为分割节点，分成 ≤36, >36 两类。  
左侧有11个样本，4个Yes，7个No。  
$Gini_L = 1 - ((4/11)^2 + (7/11)^2) = 0.4628$  
右侧有9个样本，8个Yes，1个No。  
$Gini_R = 1 - ((8/9)^2 + (1/9)^2) = 0.1975$  
加权 $Gini_{split} = \frac{11}{20} \times 0.4628 + \frac{9}{20} \times 0.1975 \approx 0.3436$  

##### **2. Income**
取分割点50000，分成 ≤50000, >50000 两类。  
左侧有12个样本，5个Yes，7个No。  
$Gini_L = 1 - ((5/12)^2 + (7/12)^2) = 0.4861$  
右侧有8个样本，7个Yes，1个No。  
$Gini_R = 1 - ((7/8)^2 + (1/8)^2) = 0.2188$  
加权 $Gini_{split} = \frac{12}{20} \times 0.4861 + \frac{8}{20} \times 0.2188 \approx 0.3792$  

##### **3. Credit Score**
取分割点650，分成 ≤650, >650 两类。  
左侧有10个样本，3个Yes，7个No。  
$Gini_L = 1 - ((3/10)^2 + (7/10)^2) = 0.42$  
右侧有10个样本，9个Yes，1个No。  
$Gini_R = 1 - ((9/10)^2 + (1/10)^2) = 0.18$  
加权 $Gini_{split} = \frac{10}{20} \times 0.42 + \frac{10}{20} \times 0.18 = 0.30$ 

##### **4. Employment Type**
分成 Part-time vs Other 两类。  
Part-time: 4个样本，0个Yes，4个No。  
$Gini_L = 0$  
Other: 16个样本，12个Yes，4个No。  
$Gini_R = 1 - ((12/16)^2 + (4/16)^2) = 0.375$  
加权 $Gini_{split} = \frac{4}{20} \times 0 + \frac{16}{20} \times 0.375 = 0.30$  

##### **5. Education**
分成 High School vs Other 两类。  
High School: 6个样本，0个Yes，6个No。  
$Gini_L = 0$  
Other: 14个样本，12个Yes，2个No。  
$Gini_R = 1 - ((12/14)^2 + (2/14)^2) \approx 0.2449$  
加权 $Gini_{split} = \frac{6}{20} \times 0 + \frac{14}{20} \times 0.2449 \approx 0.1714$  

##### **6. Home Ownership**
分成 Own vs Other 两类。  
Own: 6个样本，6个Yes，0个No。  
$Gini_L = 0$  
Other: 14个样本，6个Yes，8个No。  
$Gini_R = 1 - ((6/14)^2 + (8/14)^2) \approx 0.4898$  
加权 $Gini_{split} = \frac{6}{20} \times 0 + \frac{14}{20} \times 0.4898 \approx 0.3429$  

##### **7. Previous Loan**
分成 No vs Yes 两类。  
No: 12个样本，9个Yes，3个No。  
$Gini_L = 1 - ((9/12)^2 + (3/12)^2) = 0.375$  
Yes: 8个样本，3个Yes，5个No。  
$Gini_R = 1 - ((3/8)^2 + (5/8)^2) = 0.46875$  
加权 $Gini_{split} = \frac{12}{20} \times 0.375 + \frac{8}{20} \times 0.46875 = 0.4125$  

##### **8. Married**
分成 Yes vs No 两类。  
Yes: 12个样本，10个Yes，2个No。  
$Gini_L = 1 - ((10/12)^2 + (2/12)^2) \approx 0.2778$  
No: 8个样本，2个Yes，6个No。  
$Gini_R = 1 - ((2/8)^2 + (6/8)^2) = 0.375$  
加权 $Gini_{split} = \frac{12}{20} \times 0.2778 + \frac{8}{20} \times 0.375 \approx 0.3167$  

故根节点选择Education (High School vs Other)，Gini最小。

再看第二层. 由于左子节点 (Education = High School)中的所有样本都是No,   
$Gini = 0$, 无需再分裂. 故只用分裂右子节点. 

##### **1. Age (右子节点)**
取分割点36，分成 ≤36, >36 两类。  
左侧有5个样本，3个Yes，2个No。  
$Gini_L = 1 - ((3/5)^2 + (2/5)^2) = 0.48$  
右侧有9个样本，9个Yes，0个No。  
$Gini_R = 0$  
加权 $Gini_{split} = \frac{5}{14} \times 0.48 + \frac{9}{14} \times 0 \approx 0.1714$ 

##### **2. Income (右子节点)**
取分割点50000，分成 ≤50000, >50000 两类。  
左侧有6个样本，4个Yes，2个No。  
$Gini_L = 1 - ((4/6)^2 + (2/6)^2) \approx 0.4444$  
右侧有8个样本，8个Yes，0个No。  
$Gini_R = 0$  
加权 $Gini_{split} = \frac{6}{14} \times 0.4444 + \frac{8}{14} \times 0 \approx 0.1905$  

##### **3. Credit Score (右子节点)**
取分割点700，分成 ≤700, >700 两类。  
左侧有5个样本，4个Yes，1个No。  
$Gini_L = 1 - ((4/5)^2 + (1/5)^2) = 0.32$  
右侧有9个样本，8个Yes，1个No。  
$Gini_R = 1 - ((8/9)^2 + (1/9)^2) \approx 0.1975$  
加权 $Gini_{split} = \frac{5}{14} \times 0.32 + \frac{9}{14} \times 0.1975 \approx 0.2408$  

##### **4. Employment Type (右子节点)**

分成 Self-employed vs Other 两类。  
Self-employed: 4个样本，4个Yes，0个No。  
$Gini_L = 0$  
Other: 10个样本，8个Yes，2个No。  
$Gini_R = 1 - ((8/10)^2 + (2/10)^2) = 0.32$  
加权 $Gini_{split} = \frac{4}{14} \times 0 + \frac{10}{14} \times 0.32 \approx 0.2286$  

##### **5. Home Ownership (右子节点)**
分成 Own vs Other 两类。  
Own: 5个样本，5个Yes，0个No。  
$Gini_L = 0$  
Other: 9个样本，7个Yes，2个No。  
$Gini_R = 1 - ((7/9)^2 + (2/9)^2) \approx 0.3457$  
加权 $Gini_{split} = \frac{5}{14} \times 0 + \frac{9}{14} \times 0.3457 \approx 0.2222$  

##### **6. Previous Loan (右子节点)**
分成 No vs Yes 两类。  
No: 8个样本，8个Yes，0个No。  
$Gini_L = 0$  
Yes: 6个样本，4个Yes，2个No。  
$Gini_R = 1 - ((4/6)^2 + (2/6)^2) \approx 0.4444$  
加权 $Gini_{split} = \frac{8}{14} \times 0 + \frac{6}{14} \times 0.4444 \approx 0.1905$  

##### **7. Married (右子节点)**
分成 Yes vs No 两类。  
Yes: 10个样本，10个Yes，0个No。  
$Gini_L = 0$  
No: 4个样本，2个Yes，2个No。  
$Gini_R = 0.5$  
加权 $Gini_{split} = \frac{10}{14} \times 0 + \frac{4}{14} \times 0.5 \approx 0.1429$  

故右子节点选择Married, 最终得到的决策树如下:

- 先判断Education是否为High School
  - 若 Yes → 预测 "No"
  - 若 No → [Married?]
      - 若 Yes → 预测 "Yes" 
      - 若 No → 预测 "No" 

---
5. For the data set in https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic, try following algorithms learned in the class: a) Discriminate function (mse, ppn, svm); b) Generative method (LDA, QDA, Naive Bayes (Gaussian)); c) logistic regression, neural network; d) KNN, tree method, random forest, e): kernel method (kernel svm, Gaussian process classifier). Report the parameters of your algorithm, and the accuracy on the test set.

对UCI的威斯康星州乳腺癌诊断数据集，尝试以下算法：  
a) 判别函数（MSE、感知器、SVM）  
b) 生成方法（LDA、QDA、高斯朴素贝叶斯）  
c) 逻辑回归、神经网络  
d) KNN、决策树、随机森林  
e) 核方法（核SVM、高斯过程分类器）  
报告算法参数及测试集准确率。

**解答：**
**数据集说明：** 该数据集包含30个特征，任务是二分类（恶性/良性）。

**算法与参数：**
1. **MSE分类器**：闭式解 $\mathbf{w} = (X^\top X)^{-1} X^\top \mathbf{y}$
2. **感知器**：学习率=0.01，迭代1000次
3. **SVM**：线性核，C=1.0
4. **LDA/QDA**：假设多元高斯分布，不同协方差结构
5. **朴素贝叶斯**：特征独立的高斯分布
6. **逻辑回归**：L2正则化，C=1.0
7. **神经网络**：单隐藏层（10神经元），ReLU，Adam优化器
8. **KNN**：k=5，欧氏距离
9. **决策树**：最大深度=5，基尼分裂准则
10. **随机森林**：100棵树，最大深度=10
11. **核SVM**：RBF核，C=1.0，$\gamma=1/(n\_features \cdot \text{Var}(X))$
12. **高斯过程分类器**：RBF核，拉普拉斯近似

**评估方法：** 将数据集按70/30分割为训练集和测试集，重复5次取平均准确率。

**预期结果：**
- 简单模型（MSE、感知器）准确率较低（~85%）
- SVM、逻辑回归、随机森林表现较好（~95%-98%）
- 高斯过程分类器和核SVM在调优后可达最高准确率（~98%+）