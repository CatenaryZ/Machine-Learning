### 1 Classification

1. Derive the dual formulation of the soft support vector machine algorithm.
#### Solution of T1
软间隔SVM原始问题:  
**最小化：**  
$$\frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i$$  
**满足约束：**  
$$y_i(w \cdot x_i + b) \geq 1 - \xi_i, \xi_i \geq 0$$  
设$\alpha_i \geq 0$,$\mu_i \geq 0$，构造拉格朗日函数：
$$
L(w, b, \xi, \alpha, \mu) = \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n \alpha_i [y_i(w^T x_i + b) - 1 + \xi_i] - \sum_{i=1}^n \mu_i \xi_i
$$
对 $w$、$b$、$\xi_i$ 求偏导:
$$
\frac{\partial L}{\partial w} = w - \sum_{i=1}^n \alpha_i y_i x_i = 0 \Rightarrow w = \sum_{i=1}^n \alpha_i y_i x_i
$$
$$
\frac{\partial L}{\partial b} = -\sum_{i=1}^n \alpha_i y_i = 0 \Rightarrow \sum_{i=1}^n \alpha_i y_i = 0
$$
$$
\frac{\partial L}{\partial \xi_i} = C - \alpha_i - \mu_i = 0 \Rightarrow \alpha_i = C - \mu_i
$$
代入拉格朗日对偶函数,
$$
\begin{aligned}
\mathcal{G}(\alpha,\mu) =& \min\limits_{w,b,\xi}L(w, b, \xi, \alpha, \mu) \\
=&\frac{1}{2} \|\sum_{i=1}^n \alpha_i y_i x_i \|^2 + C \sum_{i=1}^n \xi_i \\ 
&- \sum_{i=1}^n \alpha_i [y_i(\sum_{j=1}^n \alpha_j y_j x_j^T x_i + b) - 1 + \xi_i] - \sum_{i=1}^n \mu_i \xi_i \\
=& \frac{1}{2} \left(\sum_{i=1}^n \alpha_i y_i x_i\right)^T \cdot \left(\sum_{j=1}^n \alpha_j y_j x_j\right) \\
&- \sum_{i=1}^n \alpha_i [y_i(\sum_{j=1}^n \alpha_j y_j x_j^T x_i + b) - 1]\\
=& \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j - \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j \\
& - b\sum_{i=1}^n\alpha_i y_i+ \sum_{i=1}^n \alpha_i\\
=& \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j
\end{aligned}
$$
从而得到对偶问题：
$$
\max_{\alpha,\mu}\mathcal{G}(\alpha,\mu) = \max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j
$$
约束条件为：
$$
\sum_{i=1}^n \alpha_i y_i = 0, \quad 0 \leq \alpha_i \leq C, \quad i = 1, \dots, n
$$

---
2. Derive the Discriminant function of the Quadratic Discriminant Analysis.
#### Solution of T2

QDA假设对于每一个类别k，其特征向量x服从多元高斯分布：
$$
p(x | Y = k) = \frac{1}{(2\pi)^{p/2} |\Sigma_k|^{1/2}} \exp\left(-\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)\right)
$$
记$\pi_k = P(Y=k)$，由Bayes，后验概率为：
$$
P(Y = k | x) = \frac{\pi_k p(x | Y = k)}{\sum_{l=1}^K \pi_l p(x | Y = l)}
$$
决策规则为:
$$
\begin{aligned}
\hat{Y} =& \arg\max_k P(Y = k | x) \\
=&\arg\max_k \frac{\pi_k p(x | Y = k)}{\sum_{l=1}^K \pi_l p(x | Y = l)}
\end{aligned}
$$

取对数并忽略常数项：
$$
\begin{aligned}
\hat{Y} = \arg\max_k -\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k) - \frac{1}{2} \log |\Sigma_k| + \log \pi_k 
\end{aligned}
$$
从而导出了QDA的判别函数
$$
\delta_k(\mathbf{x}) = -\frac{1}{2} (\mathbf{x} - \mu_k)^\top \Sigma_k^{-1} (\mathbf{x} - \mu_k) - \frac{1}{2} \log |\Sigma_k| + \log \pi_k
$$

---
3. Derive the Gaussian process classifier by using the Laplace approximation.

#### Solution of T3
对分类问题而言，观测目标 $y_i \in \{0, 1\}$ 离散，高斯噪声模型 $y_i = f(x_i) + \epsilon_i$ 不再适用。因此引入潜函数 $f(\mathbf{x})$ ，并利用链接函数(通常为sigmoid函数)将其映射到概率。

为潜函数赋予一个高斯过程先验:
$$
p(\mathbf{f} | X) = \mathcal{N}(\mathbf{f} | \mathbf{0}, K)
$$
其中 $K_{ij} = k(\mathbf{x}_i, \mathbf{x}_j)$ 。

给定潜函数 $\mathbf{f}$，$\mathbf{y}$是伯努利随机变量构成的随机向量
$$
p(\mathbf{y} | \mathbf{f}) = \prod_{i=1}^n p(y_i | f_i) = \prod_{i=1}^n \sigma(f_i)^{y_i} (1 - \sigma(f_i))^{1 - y_i}
$$
要求潜函数的后验分布 $p(\mathbf{f} | X, \mathbf{y})$ ,根据Bayes: $p(\mathbf{f} | X, \mathbf{y}) = \frac{p(\mathbf{y} | \mathbf{f})  p(\mathbf{f} | X)}{p(\mathbf{y} | X)}$
分母为正则化项, ${p(\mathbf{y} | X)}$ 已知。由于 $p(\mathbf{y} | \mathbf{f})$ 非高斯，因此考虑拉普拉斯近似。以下求解近似分布 $q(\mathbf{f} | X, \mathbf{y})$ 

先求后验分布的众数 $\hat{\mathbf{f}}$。对后验分布取对数，得到：
$$
\hat{\mathbf{f}} = \arg\max_{\mathbf{f}} [\log p(\mathbf{y} | \mathbf{f}) + \log p(\mathbf{f} | X)]
$$

定义 $\Psi(\mathbf{f}) \triangleq \log p(\mathbf{y} | \mathbf{f}) + \log p(\mathbf{f} | X)$
其中 $$\log p(\mathbf{f} | X) = -\frac{1}{2} \mathbf{f}^\top K^{-1} \mathbf{f} - \frac{1}{2} \log |K| - \frac{n}{2} \log 2\pi$$ 
$$
\log p(\mathbf{y} | \mathbf{f}) = \sum_{i=1}^n [y_i \log \sigma(f_i) + (1-y_i) \log (1 - \sigma(f_i))]$$
所以，
$$
\Psi(\mathbf{f}) = \log p(\mathbf{y} | \mathbf{f}) - \frac{1}{2} \mathbf{f}^\top K^{-1} \mathbf{f} + \text{const.}
$$
令 $\nabla \Psi(\mathbf{f}) = \mathbf{0}$，$\nabla \Psi(\mathbf{f}) = \nabla \log p(\mathbf{y} | \mathbf{f}) - K^{-1} \mathbf{f}$
其中， $\nabla \log p(\mathbf{y} | \mathbf{f})$  的第i个分量为：
$$
\frac{\partial}{\partial f_i} \log p(\mathbf{y} | \mathbf{f}) = y_i - \sigma(f_i)
$$
因此，
\[
\nabla \Psi(\mathbf{f}) = (\mathbf{y} - \boldsymbol{\sigma}) - K^{-1} \mathbf{f}
\]
其中 \( \boldsymbol{\sigma} = [\sigma(f_1), \dots, \sigma(f_n)]^\top \)。

这是一个非线性方程组，通常使用**牛顿-拉弗森法**等迭代优化算法求解 \( \hat{\mathbf{f}} \)。

#### 2. 计算负Hessian矩阵以构建高斯近似

在众数 \( \hat{\mathbf{f}} \) 处，我们计算 \( \Psi(\mathbf{f}) \) 的Hessian矩阵，其逆将作为近似高斯分布的协方差矩阵。

Hessian矩阵 \( \nabla \nabla \Psi(\mathbf{f}) \) 为：
\[
\nabla \nabla \Psi(\mathbf{f}) = \nabla \nabla \log p(\mathbf{y} | \mathbf{f}) - K^{-1}
\]

计算 \( \nabla \nabla \log p(\mathbf{y} | \mathbf{f}) \)：
这是一个对角矩阵，因为 \( \log p(\mathbf{y} | \mathbf{f}) = \sum_i \log p(y_i | f_i) \)，且 \( f_i \) 只影响第 \( i \) 项。
\[
\frac{\partial^2}{\partial f_i^2} \log p(y_i | f_i) = \frac{\partial}{\partial f_i} (y_i - \sigma(f_i)) = -\sigma(f_i)(1 - \sigma(f_i))
\]
因此，
\[
\nabla \nabla \log p(\mathbf{y} | \mathbf{f}) = -W
\]
其中 \( W \) 是一个对角矩阵，其对角线元素为 \( W_{ii} = \sigma(f_i)(1 - \sigma(f_i)) \)。

所以，在众数 \( \hat{\mathbf{f}} \) 处，Hessian矩阵为：
\[
H = \nabla \nabla \Psi(\hat{\mathbf{f}}) = -\hat{W} - K^{-1}
\]
其中 \( \hat{W}_{ii} = \sigma(\hat{f}_i)(1 - \sigma(\hat{f}_i)) \)。

拉普拉斯近似使用以下高斯分布来近似后验：
\[
q(\mathbf{f} | X, \mathbf{y}) = \mathcal{N}(\mathbf{f} | \hat{\mathbf{f}}, \Sigma)
\]
其中协方差矩阵 \( \Sigma = (-H)^{-1} = (\hat{W} + K^{-1})^{-1} \)。

利用Woodbury矩阵恒等式，可以将其写为计算上更便利的形式：
\[
\Sigma = (K^{-1} + \hat{W})^{-1} = K - K(K + \hat{W}^{-1})^{-1}K
\]

---

### 3. 进行预测

现在，我们想预测一个新点 \( \mathbf{x}_* \) 的标签 \( y_* \)。

#### (1) 计算潜变量 \( f_* \) 的预测分布

根据高斯过程的性质，训练潜变量 \( \mathbf{f} \) 和测试潜变量 \( f_* \) 的联合先验是高斯分布：
\[
\begin{bmatrix} \mathbf{f} \\ f_* \end{bmatrix} \sim \mathcal{N}\left( \mathbf{0}, \begin{bmatrix} K & \mathbf{k}_* \\ \mathbf{k}_*^\top & k_{**} \end{bmatrix} \right)
\]
其中 \( \mathbf{k}_* = [k(\mathbf{x}_1, \mathbf{x}_*), \dots, k(\mathbf{x}_n, \mathbf{x}_*)]^\top \)，\( k_{**} = k(\mathbf{x}_*, \mathbf{x}_*) \)。

在拉普拉斯近似下，我们使用近似后验 \( q(\mathbf{f}|X,\mathbf{y}) = \mathcal{N}(\mathbf{f} | \hat{\mathbf{f}}, \Sigma) \) 来代替真实后验 \( p(\mathbf{f}|X,\mathbf{y}) \)。

那么，\( f_* \) 的预测分布也是一个高斯分布：
\[
q(f_* | X, \mathbf{y}, \mathbf{x}_*) = \int p(f_* | X, \mathbf{x}_*, \mathbf{f})  q(\mathbf{f} | X, \mathbf{y})  d\mathbf{f} = \mathcal{N}(f_* | \mu_*, \sigma^2_*)
\]

其均值和方差为：
- **均值**：
  \[
  \mu_* = \mathbf{k}_*^\top K^{-1} \hat{\mathbf{f}}
  \]
  在实践中，我们通常定义一个向量 \( \hat{\mathbf{a}} = K^{-1} \hat{\mathbf{f}} \)，它是牛顿-拉弗森法中求众数时的一个中间结果。所以 \( \mu_* = \mathbf{k}_*^\top \hat{\mathbf{a}} \)。

- **方差**：
  \[
  \sigma^2_* = k_{**} - \mathbf{k}_*^\top (K + \hat{W}^{-1})^{-1} \mathbf{k}_*
  \]

#### (2) 计算类别概率

最终的预测是类别概率 \( \pi_* \triangleq P(y_*=1 | X, \mathbf{y}, \mathbf{x}_*) \)：
\[
\pi_* = \int \sigma(f_*)  q(f_* | X, \mathbf{y}, \mathbf{x}_*)  df_*
\]

---
1. For the following data, build a tree by using: a) Gini index, b) the depth of tree is two. The target variable is **Loan Approved**.


**解答：**
使用表中提供的20条数据，计算每个特征的基尼指数，选择最优分裂特征。

**步骤：**
1. 计算根节点的基尼指数：
   - 批准：12条，拒绝：8条
   - $Gini = 1 - \left(\frac{12}{20}\right)^2 - \left(\frac{8}{20}\right)^2 = 0.48$

2. 对每个特征计算基尼指数增益，例如“Income”：
   - 按中位数分裂（约50,000）：
     - 高收入：批准10/10，拒绝0/10 → Gini = 0
     - 低收入：批准2/10，拒绝8/10 → Gini = 0.32
   - 加权平均Gini = $0.5 \times 0 + 0.5 \times 0.32 = 0.16$
   - 增益 = $0.48 - 0.16 = 0.32$

3. 比较所有特征（Income、Credit Score、Employment Type等），选择增益最大的特征作为根节点分裂。

4. 对每个子节点重复上述过程，直到深度达到2。

**最终树结构示例：**
- 根节点：Income ≤ 50,000?
  - 是（低收入）→ 子节点：Credit Score ≤ 650?
    - 是 → 拒绝（多数）
    - 否 → 批准（多数）
  - 否（高收入）→ 子节点：Employment Type?
    - 全职/自雇 → 批准
    - 兼职/失业 → 拒绝

---
5. For the data set in https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic, try following algorithms learned in the class: a) Discriminate function (mse, ppn, svm); b) Generative method (LDA, QDA, Naive Bayes (Gaussian)); c) logistic regression, neural network; d) KNN, tree method, random forest, e): kernel method (kernel svm, Gaussian process classifier). Report the parameters of your algorithm, and the accuracy on the test set.

对UCI的威斯康星州乳腺癌诊断数据集，尝试以下算法：  
a) 判别函数（MSE、感知器、SVM）  
b) 生成方法（LDA、QDA、高斯朴素贝叶斯）  
c) 逻辑回归、神经网络  
d) KNN、决策树、随机森林  
e) 核方法（核SVM、高斯过程分类器）  
报告算法参数及测试集准确率。

**解答：**
**数据集说明：** 该数据集包含30个特征，任务是二分类（恶性/良性）。

**算法与参数：**
1. **MSE分类器**：闭式解 $\mathbf{w} = (X^\top X)^{-1} X^\top \mathbf{y}$
2. **感知器**：学习率=0.01，迭代1000次
3. **SVM**：线性核，C=1.0
4. **LDA/QDA**：假设多元高斯分布，不同协方差结构
5. **朴素贝叶斯**：特征独立的高斯分布
6. **逻辑回归**：L2正则化，C=1.0
7. **神经网络**：单隐藏层（10神经元），ReLU，Adam优化器
8. **KNN**：k=5，欧氏距离
9. **决策树**：最大深度=5，基尼分裂准则
10. **随机森林**：100棵树，最大深度=10
11. **核SVM**：RBF核，C=1.0，$\gamma=1/(n\_features \cdot \text{Var}(X))$
12. **高斯过程分类器**：RBF核，拉普拉斯近似

**评估方法：** 将数据集按70/30分割为训练集和测试集，重复5次取平均准确率。

**预期结果：**
- 简单模型（MSE、感知器）准确率较低（~85%）
- SVM、逻辑回归、随机森林表现较好（~95%-98%）
- 高斯过程分类器和核SVM在调优后可达最高准确率（~98%+）