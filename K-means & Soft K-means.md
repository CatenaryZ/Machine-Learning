## **K-Means 与 软K-Means 算法学习笔记**

### **第一部分：K-Means（硬聚类）**

#### **1. 核心思想**
K-Means 是一种**无监督学习**的**硬聚类**算法。
*   **硬聚类**：每个数据点**有且仅有**一个簇归属。
*   **目标**：将数据集划分为 K 个簇，使得簇内数据点尽可能相似（方差小），簇间数据点尽可能不同。

#### **2. 算法步骤**
1.  **初始化**：随机选择 K 个数据点作为初始的**簇质心**（Centroids）。
2.  **迭代**：重复以下步骤直到质心不再发生显著变化（收敛）：
    *   **分配步骤**：对于每个数据点 \( x_i \)，计算它与所有 K 个质心 \( \mu_k \) 的欧氏距离，并将其分配给**距离最近**的质心所在的簇。
        \[
        \text{簇标签} = \arg\min_{k} ||x_i - \mu_k||^2
        \]
    *   **更新步骤**：对于每个簇 \( k \)，重新计算其质心。新的质心是该簇所有数据点的**均值**。
        \[
        \mu_k^{(new)} = \frac{1}{|C_k|} \sum_{x_i \in C_k} x_i
        \]
        其中 \( C_k \) 是第 \( k \) 个簇的集合， \( |C_k| \) 是该簇中点的数量。

#### **3. 关键点与局限性**
*   **优点**：简单、高效、适用于大规模数据集。
*   **缺点**：
    *   **对初始质心敏感**：可能收敛到局部最优。
    *   **必须指定K值**。
    *   **对噪声和异常值敏感**（因为均值计算会受极端值影响）。
    *   **只能处理球状簇**。
    *   **硬分配的僵化**：对于处在两个簇边界上的点，强制将其归为一类可能不合理，因为它忽略了该点属于另一个簇的可能性。

---

### **第二部分：软K-Means（软聚类）**

#### **1. 核心思想**
软K-Means 是 K-Means 的一个自然扩展，它是一种**软聚类**算法。
*   **软聚类**：每个数据点可以**同时属于所有簇**，只是属于不同簇的**程度（概率或权重）**不同。
*   **目标**：不再是简单分配，而是计算一个 **“归属度矩阵”**，其中每个元素表示一个点对一个簇的归属程度。

#### **2. 核心概念：归属度**
在软K-Means中，我们引入一个关键变量：**归属度 \( r_{ik} \)**。
*   \( r_{ik} \) 表示数据点 \( x_i \) 属于簇 \( k \) 的**程度**。
*   对于一个点 \( x_i \)，其对于所有 K 个簇的归属度之和为 1：
    \[
    \sum_{k=1}^{K} r_{ik} = 1
    \]
*   这可以看作是一个**概率分布**，\( r_{ik} \) 可以解释为“点 \( i \) 属于簇 \( k \) 的概率”。

#### **3. 算法步骤（与K-Means对比）**

1.  **初始化**：随机初始化 K 个质心 \( \mu_k \)。同时，引入一个**刚度参数** \( \beta \)（可以理解为方差的倒数，\( \beta = \frac{1}{\sigma^2} \)）。\( \beta \) 控制簇的“硬度”，\( \beta \to \infty \) 时，软K-Means退化为硬K-Means。

2.  **迭代**：重复以下步骤直到收敛：
    *   **“软”分配步骤**：计算每个点 \( x_i \) 对每个质心 \( \mu_k \) 的归属度 \( r_{ik} \)。
        我们使用**软最大化函数**（Softmax），归属度与点到质心距离的负指数成正比：
        \[
        r_{ik} = \frac{\exp(-\beta ||x_i - \mu_k||^2)}{\sum_{j=1}^{K} \exp(-\beta ||x_i - \mu_j||^2)}
        \]
        *   **分母**是一个归一化项，确保所有 \( r_{ik} \) 加起来等于 1。
        *   点离一个质心越近，它对该簇的归属度就越高。
        *   如果点离所有质心都很远，则它对所有簇的归属度会趋于平均（各 ~1/K）。

    *   **更新步骤**：重新计算每个质心 \( \mu_k \)。新的质心是所有数据点的**加权平均**，权重就是各点对该簇的归属度 \( r_{ik} \)。
        \[
        \mu_k^{(new)} = \frac{\sum_{i=1}^{N} r_{ik} x_i}{\sum_{i=1}^{N} r_{ik}}
        \]
        *   一个点对当前簇的“贡献”大小，取决于它有多“属于”这个簇。

#### **4. 关键点与优势**
*   **更丰富的信息**：结果不仅给出了聚类，还给出了聚类的**置信度**。例如，一个点的归属度是 `[0.9, 0.1, 0.0]` 比 `[0.4, 0.35, 0.25]` 要确定得多。
*   **对边界点更鲁棒**：处于簇边界上的点不会被强行分割，而是被合理地赋予两个簇的归属度。
*   **是高斯混合模型的基础**：软K-Means 可以看作是高斯混合模型的一个特例（假设所有簇的方差相同且为球形）。GMM 是更强大的软聚类模型。

---

### **第三部分：对比总结**

| 特征 | **K-Means (硬聚类)** | **软K-Means (软聚类)** |
| :--- | :--- | :--- |
| **核心思想** | 非此即彼，硬分配 | 亦此亦彼，软分配 |
| **归属表示** | 二值标签，`0` 或 `1` | 连续权重，`r_ik` ∈ [0, 1] |
| **分配步骤** | `argmin` 寻找最近质心 | `Softmax` 计算归属概率 |
| **更新步骤** | 计算簇内点的**算术平均** | 计算所有点的**加权平均** |
| **结果信息** | 每个点属于哪个簇 | 每个点属于每个簇的**概率** |
| **对边界点** | 强制归入一类，可能不合理 | 合理分配概率，结果更平滑 |
| **计算复杂度** | 相对较低 | 相对较高（因为要计算所有点对的所有归属度） |
| **关系** | 是软K-Means在 \( \beta \to \infty \) 时的特例 | 是K-Means的泛化，也是GMM的思想先驱 |

---

### **第四部分：一个简单的例子**

假设有两个质心 \( \mu_1 \) 和 \( \mu_2 \)，一个数据点 \( x \) 到它们的距离分别为 \( d_1 = 1 \)， \( d_2 = 3 \)。设 \( \beta = 1 \)。

*   **K-Means**：
    *   因为 \( d_1 < d_2 \)，所以点 \( x \) 被**硬分配**给簇 1。标签为 `[1, 0]`。

*   **软K-Means**：
    *   计算归属度：
        \[
        r_1 \propto \exp(-1^2) = \exp(-1) \approx 0.368
        \]
        \[
        r_2 \propto \exp(-3^2) = \exp(-9) \approx 0.000123
        \]
        \[
        \text{归一化：} Z = 0.368 + 0.000123 \approx 0.368
        \]
        \[
        r_1 = 0.368 / 0.368 \approx 0.9997
        \]
        \[
        r_2 = 0.000123 / 0.368 \approx 0.0003
        \]
    *   点 \( x \) 的归属度为 `[0.9997, 0.0003]`。它**几乎完全**属于簇 1，但也有极微小的可能性属于簇 2。这真实地反映了数据的分布情况。

