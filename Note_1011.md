**第1页**  
Lecture 4.2: 非参数方法  
谢丹  
清华大学数学系  
2025年10月13日  

**第2页**  
参数化方法 vs. 非参数化方法  
从基于假设的方法到数据驱动的方法  
参数化方法  
判别函数：均方误差，感知器，支持向量机  
生成模型：线性判别分析，二次判别分析，朴素贝叶斯  
判别模型：逻辑回归  
模型结构固定，具有参数 w  
训练优化 E(w)（可能带有约束，例如支持向量机）  
非参数化方法  
K最近邻  
决策树  
随机森林  
不对函数形式做强假设  
模型复杂度随数据量增长  

**第3页**  
什么是K最近邻？  
核心思想  
简单直观的机器学习算法  
可用于分类和回归  
基于实例的学习（惰性学习）  
"告诉我你的邻居是谁，我就告诉你你是谁"  
例子  
分类一个新水果：查看你已经知道的3个最相似的水果。如果2个是苹果，1个是橙子，则猜测它是苹果。  

**第4页**  
K最近邻算法步骤  
1. 选择邻居数量 **K**  
2. 计算到所有训练点的距离  
3. 识别K个最近邻居  
4. 对邻居进行投票（分类采用多数票决）  
5. 进行预测  
注意：此处的 \( K \) 不是类别数量，而是一个需要选择的参数。  

**第5页**  
距离度量  
欧氏距离  
\[d = \sqrt{\sum_{i=1}^{D} (x_i - y_i)^2}\]  
最常用，直线距离  
曼哈顿距离  
\[d = \sum_{i=1}^{D} |x_i - y_i|\]  
网格状距离  
闵可夫斯基距离  
\[d = \left( \sum_{i=1}^{D} |x_i - y_i|^p \right)^{1/p}\]  
广义形式  
汉明距离  
用于分类数据 - 计算不同位置的数目  

**第6页**  
偏差-方差权衡  
小K值（K=1）  
- **低偏差**  
- **高方差**  
- 复杂决策边界  
- 过拟合  
大K值（K=99）  
- **高偏差**  
- **低方差**  
- 平滑决策边界  
- 欠拟合  

**第7页**  
实际K值选择  
选择K的指南  
- 通常使用**奇数**以避免平票  
- 常见起点：\( K = \sqrt{n} \)，其中 \( n \) 是样本量  
- 使用**交叉验证**寻找最优K  
- 考虑**数据集大小**和**噪声水平**  
例子  
对于有1000个样本的数据集：\(\sqrt{1000} \approx 32\)，因此尝试 \( K=31, 33, 35 \) 等。  

**第8页**  
分类 vs 回归  
分类  
多数投票  
- 每个邻居为其类别投票  
- 最频繁的类别获胜  
- 可以使用加权投票  
例子：\( K=5 \)，投票：[A, A, B, A, B] → 预测：A  
回归  
加权平均  
- 邻居值的平均值  
- 可以使用距离加权平均  
- 更近的邻居有更大影响  
例子：\( K=3 \)，值：[10, 12, 11] → 预测：11  

**第9页**  
特征缩放的重要性  
关键步骤！  
在应用K最近邻之前必须缩放特征  
例子  
未缩放：  
► 薪水：30,000-100,000  
► 年龄：20-70  
► 薪水主导了距离计算！  
例子  
缩放后：  
► 标准化  
► 归一化  
► 所有特征贡献相等  
常用缩放方法：StandardScaler, MinMaxScaler, RobustScaler  

**第10页**  
优缺点  
优点  
- **简单**易懂易实现  
- **无训练阶段** - "训练"快速  
- **轻松适应**新数据  
- **通用** - 分类和回归  
- 对数据分布**无假设**  
缺点  
- 预测时**计算开销大**  
- 对**不相关特征敏感**  
- **维度灾难**  
- **内存密集** - 存储所有数据  
- 对**异常值敏感**  

**第11页**  
什么是决策树？  
定义  
一种非参数监督学习方法，从数据特征中学习简单的决策规则以预测目标变量。  
天气？  
湿度？  
是  
否  
是  
幽默  
是  
是  
是  
是  
图例：我们该打网球吗？  

**第12页**  
树结构解析  
组成部分  
- **根节点**：第一个特征测试  
- **内部节点**：中间决策  
- **分支**：测试结果  
- **叶节点**：最终分类  
关键概念  
- **递归分区**  
- **纯度度量**  
- **停止准则**  

**第13页**  
测量节点不纯度  
目标  
找到能最小化子节点不纯度的划分。  
基尼不纯度  
\[I_G(p) = 1 - \sum_{i=1}^K p_i^2\]  
熵  
\[I_E(p) = -\sum_{i=1}^K p_i \log_2 p_i\]  
- 范围：二分类时为 [0, 0.5]  
- 基于信息论  
- 偏好更大的分区  
- 衡量无序度  
\( p_i \) 是类别 \( i \) 的概率，定义为  
\[p_i = \frac{N_i}{N}\]  

**第14页**  
划分的不纯度计算  
跨子节点平均不纯度  
一个划分的总体不纯度计算为子节点不纯度的加权平均：  
\[I_{\text{split}} = \frac{N_{\text{left}}}{N} \cdot I_{\text{left}} + \frac{N_{\text{right}}}{N} \cdot I_{\text{right}}\]  
其中：  
- \( I_{\text{left}}, I_{\text{right}} \)：左右子节点的不纯度度量  
- \( N_{\text{left}}, N_{\text{right}} \)：每个子节点的样本数  
- \( N = N_{\text{left}} + N_{\text{right}} \)：总样本数  

**第15页**  
分类与回归树  
算法大纲  
1. 所有数据起始于根节点  
2. 对每个特征，找到最佳划分阈值  
3. 选择具有最小不纯度的特征  
4. 递归地划分子节点  
5. 当满足停止准则时停止  
叶节点的决策由具有最大概率的类别给出。  

**第16页**  
特征类型处理  
数值特征  
- **二元划分**：\( x_j \leq t \) 对比 \( x_j > t \)  
- 排序数值，测试中点  
- 高效：每个特征 \( O(n \log n) \)  
分类特征  
- **二元**：类别 \(\in S\) 对比 \(\notin S\)  
- **多路**：每个类别一个分支  
- 对于高基数类别可能导致过拟合  

**第17页**  
何时停止划分？  
预剪枝（提前停止）  
- 最大树深度  
- 叶节点最小样本数  
- 划分所需最小样本数  
- 最小不纯度减少量  
后剪枝（代价复杂度）  
最小化：  
\[R_{\alpha}(T) = R(T) + \alpha |T|\]  
其中：  
- \( R(T) \)：误分类率  
- \( |T| \)：叶节点数量  
- \(\alpha\)：复杂度参数  
典型值  
- max_depth: 3-10  
- min_samples_leaf: 1-20  
- min_samples_split: 2-20  
过程  
1. 生长完整树  
2. 贪婪地折叠节点  
3. 通过交叉验证选择 \(\alpha\)  

**第18页**  
网球运动例子  
（表格数据：Outlook, Temp, Humidity, Windy, Play）  
根节点计算  
父节点：9 是，5 否 → 基尼不纯度 = \(1 - (9/14)^2 - (5/14)^2 = 0.459\)  

**第19页**  
划分计算  
天气划分  
- 晴朗：[2 是, 3 否] → 基尼 = 0.48  
- 阴天：[4 是, 0 否] → 基尼 = 0  
- 雨天：[3 是, 2 否] → 基尼 = 0.48  
- 加权：  
  \[  \frac{5}{14} \times 0.48 + \frac{4}{14} \times 0 + \frac{5}{14} \times 0.48 = 0.343\]  
有风划分  
- 否：[6 是, 2 否] → 基尼 = 0.375  
- 是：[3 是, 3 否] → 基尼 = 0.5  
- 加权：  
  \[  \frac{8}{14} \times 0.375 + \frac{6}{14} \times 0.5 = 0.429\]  
湿度划分  
- 高：[3 是, 4 否] → 基尼 = 0.490  
- 正常：[6 是, 1 否] → 基尼 = 0.245  
- 加权：  
  \[  \frac{7}{14} \times 0.490 + \frac{7}{14} \times 0.245 = 0.367\]  
最佳划分  
天气划分的不纯度最小  
- 首先按此划分！  

**第20页**  
最终网球决策树  
湿度？  
高  
否  
正常  
是  
天气？  
阴天  
是  
雨天  
是  
有风？  
否  
是  
解释  
► 如果阴天 → 总是打球  
► 如果晴朗 → 检查湿度  
► 如果雨天 → 检查风  

**第21页**  
对于连续变量，特征空间被划分为不同的区域。  
（图示：基于 \(x_1\) 和 \(x_2\) 的阈值划分出区域1、2、3、4）  

**第22页**  
优缺点  
优点  
- **可解释性强**：易于解释  
- **假设少**：处理混合数据  
- **非参数**：无分布假设  
- **特征选择**：内置重要性评估  
- **稳健**：能较好处理异常值  
局限性  
- **高方差**：对微小变化不稳定  
- **过拟合**：复杂树泛化能力差  
- **贪婪**：局部最优  
- **轴对齐**：对对角线边界效果差  
- **有偏**：偏好具有更多水平的特征  
解决方案：集成方法  
随机森林和梯度提升克服了这些局限性！  

**第23页**  
随机森林：直观理解  
群体智慧  
集成方法，组合多个决策树  
每棵树在不同的数据子集和特征子集上训练  
最终预测：多数投票（分类）或平均（回归）  
通过随机化减少过拟合  

**第24页**
随机森林伪代码
```
1: procedure RANDOMFOREST(D, T, m)
2:   输入: 训练数据 D, 树的数量 T, 特征子集大小 m
3:   输出: 集成模型 F
4:   F ← ∅                  ▷ 初始化空森林
5:   for t = 1 to T do
6:       D_t ← BootstrapSample(D)    ▷ 有放回抽样
7:       Tree_t ← GrowTree(D_t, m)
8:       F ← F ∪ {Tree_t}
9:   end for
10:  return F
11: end procedure
```

树生长过程
```
1: procedure GROWTREE(D, m)
2:   if StoppingCondition(D) then
3:       return CreateLeafNode(D)
4:   else
5:       features ← RandomSubset(all features, m)
6:       best_split ← FindBestSplit(D, features)
7:       left ← GrowTree(D_left)
8:       right ← GrowTree(D_right)
9:       return CreateDecisionNode(best_split, left, right)
10:  end if
11: end procedure
```
停止条件:
- 达到最大树深度
- 达到叶节点最小样本数  
- 不纯度无改善
- 所有样本属于同一类别

预测阶段
```
1: procedure PREDICT(F, x)
2:   输入: 森林 F, 实例 x
3:   输出: 预测 ŷ
4:   predictions ← ∅
5:   for tree ∈ F do
6:       pred ← tree.predict(x)
7:       predictions ← predictions ∪ {pred}
8:   end for
9:   if 分类 then
10:      return mode(predictions)     ▷ 多数投票
11:  else                            ▷ 回归
12:      return mean(predictions)     ▷ 取平均值
13:  end if
14: end procedure
```