*这是一个课堂随记,仅供作者本人参考*
# 维数下降笔记整理

## 两种方法:PCA与LDA

主成分分析(PCA)和线性判别分析(LDA)是机器学习和数据科学中两种非常重要但又截然不同的降维技术。

---

### 主成分分析（PCA）

#### 1. 核心思想
PCA是一种**无监督学习**算法。它的目标是在**不考虑任何标签**的情况下，找到数据中方差最大的方向（即主成分），并将数据投影到这些方向上，从而实现降维。

简单比喻：想象一个三维的椭球状数据点云，PCA要做的就是找到最“长”的那个轴（第一主成分），然后是次长的且与第一个轴垂直的轴（第二主成分），依此类推。这些新轴就是数据的主要特征。

#### 2. 主要目标
- **降维**：减少特征数量，同时保留数据中最重要的信息（方差）。
- **去相关**：转换后的新特征（主成分）之间是线性无关的。
- **可视化**：将高维数据降至2维或3维，以便于我们人类观察和理解。

#### 3. 工作原理
1.  **数据标准化**：通常需要将每个特征标准化为均值为0，方差为1，以防止量纲不同的特征主导结果。
2.  **计算协方差矩阵**：协方差矩阵描述了数据中各特征之间的线性关系。
3.  **特征值分解**：计算协方差矩阵的特征值和对应的特征向量。
    - **特征向量**：代表了数据方差最大的方向，即主成分的方向。
    - **特征值**：代表了对应特征向量方向上的方差大小。特征值越大，说明该方向上的方差越大，包含的信息越多。
4.  **选择主成分**：将特征值从大到小排序，选择前k个最大的特征值对应的特征向量，构成一个新的投影矩阵。
5.  **数据转换**：将原始数据投影到这个新的k维子空间上，得到降维后的数据。

#### 4. 优点
- 能有效降低数据维数，减少计算复杂度。
- 完全无参数，只依赖于数据本身。
- 可以消除特征之间的相关性。

#### 5. 缺点
- 由于是无监督的，它不考虑任何类别标签，因此降维后的特征可能对分类任务并不是最优的。
- 主成分的解释性较差，它们是原始特征的线性组合，物理含义不明确。

---

### 线性判别分析（LDA）

#### 1. 核心思想
LDA是一种**有监督学习**算法。它的目标是在**考虑类别标签**的情况下，找到一个投影轴，使得当数据投影到这个轴上时，**类间距离尽可能大，类内距离尽可能小**。

简单比喻：想象有两堆不同颜色的点（两类数据）分布在二维平面上。LDA要找到一条直线，当把所有点投影到这条直线上时，两堆点的中心距离尽可能远，同时每一堆点自身尽可能紧凑地聚集在一起。

#### 2. 主要目标
- **特征降维**：主要用于分类任务前的降维。
- **分类**：虽然本身是一个降维技术，但也可以直接用于分类（通过计算新样本在投影空间中的类别）。

#### 3. 工作原理
1.  **计算三类散度矩阵**：
    - **类内散度矩阵（Within-class Scatter Matrix, Sw）**：衡量每个类别内部数据的分散程度。希望它越小越好。
    - **类间散度矩阵（Between-class Scatter Matrix, Sb）**：衡量不同类别中心之间的分散程度。希望它越大越好。
2.  **构建目标函数**：LDA的目标是找到一个投影方向 **w**，使得投影后的数据满足类间散度与类内散度的比值最大化。这个比值通常称为**Fisher准则**：
    `J(w) = (w^T * Sb * w) / (w^T * Sw * w)`
3.  **求解**：通过求解广义特征值问题 `(Sb) * w = λ * (Sw) * w`，得到最优的投影方向 **w**（即特征向量）。选择前k个最大的特征值对应的特征向量构成投影矩阵。
    - 对于C个类别，LDA最多能降到 **C-1** 维。例如，对于二分类问题，只能降到一个维度（一条直线）。
4.  **数据转换**：将原始数据投影到新的判别子空间上。

#### 4. 优点
- 充分利用了类别信息，因此对于分类任务，降维效果通常比PCA更好。
- 能提升分类模型的性能。

#### 5. 缺点
- 是有监督算法，需要标签。
- 依赖于数据的全局结构，对异常值敏感。
- 假设数据服从正态分布，且每个类别的协方差矩阵相同（线性假设）。

---

### PCA vs. LDA：核心区别总结

| 特性 | PCA（主成分分析） | LDA（线性判别分析） |
| :--- | :--- | :--- |
| **学习类型** | **无监督** | **有监督** |
| **核心目标** | **最大化方差** | **最大化类间距离，最小化类内距离** |
| **关注点** | 数据的**内在结构**和**全局方差** | 数据的**判别信息**和**类别可分性** |
| **应用场景** | 数据压缩、去噪、可视化 | **分类任务前的降维** |
| **降维维度** | 理论上无限制（取决于特征数） | 最多降到 **“类别数-1”** 维 |
| **结果解释** | 主成分是方差最大的方向，物理意义模糊 | 投影轴是具有最强判别力的方向，与分类直接相关 |

### 一个生动的例子

假设我们有一个数据集，记录了不同品种的狗和猫的体重和身长。

- **使用PCA**：
    - PCA会找到一个新方向，这个方向是体重和身长的某种组合，使得所有数据点（无论是狗还是猫）在这个方向上的分布最“散开”（方差最大）。它可能会找到一个方向，大致反映了动物的“体型大小”。
    - 结果可能是大狗和小狗分得很开，但大狗和大猫可能混在一起。

- **使用LDA**：
    - LDA会找到一个新方向，这个方向能最好地将狗和猫这两个类别分开。它会找到一个方向，使得所有狗的数据点尽可能聚集，所有猫的数据点也尽可能聚集，同时狗的中心和猫的中心离得尽可能远。
    - 结果可能是狗和猫被清晰地分在了投影轴的两端。

### 总结

- 当你**不了解数据类别**，或者降维的**目的不是为了分类**（而是为了可视化、压缩等），**PCA**是一个通用且强大的选择。
- 当你**明确知道数据的类别**，并且降维的**核心目的是为了提高分类性能**时，**LDA**通常是更优的选择。

在实际应用中，很多人会同时尝试两种方法，并通过交叉验证来评估哪种方法对最终任务（如分类）更有效。

## 非线性维数下降(下降结果为一个流形)


常用的非线性降维或流形学习技术有四种:t-SNE,UMAP,Isomap,Autoencoders(常用)

### 1. t-SNE (t-Distributed Stochastic Neighbor Embedding)

**核心思想**：t-SNE专注于**保留数据的局部结构**（即相似点之间的相对距离），特别适合高维数据的可视化。

**工作原理**：
1. **高维空间相似度**：计算高维空间中数据点两两之间的相似度（使用高斯分布概率）。相似的点概率高，不相似的点概率低。
2. **低维空间相似度**：在低维映射中（通常是2D或3D），同样计算数据点之间的相似度（但使用**t-分布**，尾部更重）。
3. **最小化分布差异**：通过梯度下降，调整低维空间中点的位置，使得高维和低维的两个相似度分布尽可能相似（最小化KL散度）。

**关键特点**：
- **擅长可视化**：能将高维中相似的点聚集为清晰的“簇”。
- **保留局部结构**：邻近点的关系保持得很好。
- **不保留全局结构**：簇间距离无意义。一个远处的大簇和一个近处的小簇可能没有大小关系。
- **计算昂贵**：通常只用于可视化，不适合作为特征提取的预处理步骤。
- **超参数敏感**：困惑度等参数对结果影响大。

**主要应用**：高维数据（如MNIST手写数字、基因表达数据）的探索性数据分析和可视化。

---

### 2. UMAP (Uniform Manifold Approximation and Projection)

**核心思想**：UMAP假设数据均匀分布在拓扑空间中，它试图在低维空间中**同时保留数据的局部和全局结构**。

**工作原理**（基于拓扑学）：
1. **构建图结构**：在高维空间中，为每个点找一定数量的最近邻，构建一个加权图（相似点相连）。
2. **优化低维图**：在低维空间中寻找一个图的布局，使得这个图的拓扑结构与高维图尽可能相似。

**关键特点**：
- **兼顾局部与全局**：比t-SNE能更好地保持数据的全局结构（如簇间关系）。
- **速度更快**：计算效率通常远高于t-SNE，能处理更大规模的数据集。
- **可扩展性**：既可用于可视化，也可用于降维作为其他机器学习任务的预处理。
- **理论坚实**：基于坚实的拓扑学理论（黎曼几何和纤维丛理论）。

**主要应用**：正迅速成为高维数据可视化和中等维度降维的首选工具。

---

### 3. Isomap (Isometric Mapping)

**核心思想**：Isomap是一种**基于测地距离**的流形学习算法。它不像PCA那样假设数据是线性的，而是假设数据分布在一个潜在的弯曲流形上。

**工作原理**：
1. **构建邻域图**：为每个点找到其k-最近邻，并连接形成图。
2. **计算测地距离**：计算图中所有点对之间的**最短路径距离**（而不是欧氏距离）。这个最短路径近似于流形表面的“真实”距离。
3. **多维缩放**：使用这个测地距离矩阵，应用MDS算法找到低维嵌入，使得低维空间中的欧氏距离尽可能接近高维的测地距离。

**关键特点**：
- **处理非线性流形**：能“展开”弯曲的流形（如著名的“瑞士卷”数据集）。
- **保留全局几何**：试图保持所有点对之间的测地距离。
- **对参数敏感**：邻域大小k的选择对结果影响巨大。
- **计算瓶颈**：计算所有点对的最短路径非常耗时，不适合大数据集。

**主要应用**：处理已知具有非线性流形结构的数据，如三维物体在不同视角下的图像。

---

### 4. Autoencoders (自编码器)

**核心思想**：使用**神经网络**学习一个压缩表示（编码），然后从中重建原始数据（解码）。*个人理解:神经网络本质是一个函数模拟器.我们就是用这个函数来写出流形的表达式*

**工作原理**：
1. **编码器**：一个神经网络，将高维输入数据映射到低维的“瓶颈层”（编码）。
2. **瓶颈层**：低维表示，包含了重建原始数据所需的最关键信息。
3. **解码器**：另一个神经网络，从瓶颈层的编码中尝试重建原始输入。
4. **训练**：通过最小化重建误差（如MSE）来同时训练编码器和解码器。

**关键特点**：
- **高度灵活**：网络结构可以非常灵活（CNN用于图像，RNN用于序列），能捕捉复杂的非线性模式。
- **端到端学习**：直接从数据中学习最佳的低维表示。
- **可扩展性**：能够处理极其复杂和大规模的数据。
- **生成能力**：变分自编码器（VAE）等变体可以生成新数据。
- **计算需求**：需要大量数据和计算资源来训练。
- **黑箱问题**：结果可能难以解释。

**主要应用**：图像去噪、数据压缩、特征学习、生成模型等。

* **p.s.** Autoencoder是一个非概率的生成模型

---

### 对比总结

| 方法 | 核心思想 | 优点 | 缺点 | 最佳场景 |
|------|----------|------|------|----------|
| **PCA** | 线性投影，最大化方差 | 简单、快速、可解释 | 只能捕捉线性关系 | 线性数据、去相关、作为初步分析 |
| **t-SNE** | 保持局部相似性 | 出色的聚类可视化 | 慢、不保留全局结构、仅用于可视化 | 高维数据探索性分析 |
| **UMAP** | 保持局部和全局拓扑 | 快、保留更多结构、可扩展 | 理论复杂、参数仍需调整 | 可视化及中等维度降维 |
| **Isomap** | 保持测地距离 | 能处理非线性流形 | 对参数敏感、计算量大 | 已知非线性流形结构的数据 |
| **Autoencoders** | 神经网络压缩与重建 | 极其灵活、功能强大 | 需要大量数据、训练复杂 | 复杂数据（图像、文本）、特征学习、生成任务 |

### 如何选择？

- **快速线性降维**：PCA
- **数据可视化**：**UMAP**（首选）或 t-SNE
- **处理已知的非线性流形**：Isomap
- **处理复杂数据（如图像）并需要特征学习**：**Autoencoders**
- **作为其他机器学习模型的预处理**：PCA 或 UMAP

这些方法共同构成了从简单线性到复杂非线性的降维工具箱，让您可以根据数据特性和任务目标选择最合适的技术。