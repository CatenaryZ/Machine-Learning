*这是一个课堂随记,仅供作者本人参考*
### Squared Error Loss
### Conditional Risk:
$$R(a|x) = E_{y\sim p(y|x)}[|y-a|^2]$$
### 非对称损失函数:Medical Diagnosis
* False negative
* False positive

Loss function determines the optimal decision:
* 0-1 : Mode
* Squared Loss : Mean
* Absolute Loss: Median 

# Section 2
核心是为了Generalization
### Two paths to Failure
* Underfitting (High Bias)
* Overfitting (High Variance) (比较subtle, hard to detect)


## **偏差-方差分解的数学公式表达**

### 1. 设定场景与符号

首先，我们需要定义一个清晰的数学场景：

*   **预测目标**：对于一个给定的输入 \( x \)，我们想预测一个输出 \( y \)。存在一个真实的、潜在的函数关系 \( y = f(x) + \varepsilon \)。
*   **真实函数**：\( f(x) \) 是我们永远无法知道，但试图去学习的真实函数。
*   **噪声**：\( \varepsilon \) 是均值为零、方差为 \( \sigma^2 \) 的随机噪声，即 \( \mathbb{E}[\varepsilon] = 0 \)， \( \text{Var}(\varepsilon) = \sigma^2 \)。这代表了“不可消除的误差”。
*   **我们的模型**：我们使用一个学习算法，在一個**特定的训练集** \( D \) 上训练，得到一个模型 \( \hat{f}_D(x) \)。这个模型是对 \( f(x) \) 的估计。注意，如果换一个训练集 \( D \)，我们会得到不同的 \( \hat{f}_D(x) \)。
*   **损失函数**：我们使用**平方误差** 作为衡量预测好坏的指标。在给定点 \( x \) 的误差为 \( (y - \hat{f}_D(x))^2 \)。

我们的目标，是推导出这个平方误差的**期望值**。这个期望值是对以下两个随机性的平均：
1.  对**不同训练集** \( D \) 的随机性求平均。
2.  对**数据噪声** \( \varepsilon \) 的随机性求平均。

这个期望值就是我们的“期望泛化误差”。

---

### 2. 公式推导

我们现在来推导对于一个新数据点 \( x \) 的期望预测误差：

\[
\begin{aligned}
\text{期望误差} &= \mathbb{E}_{D, \varepsilon} \left[ (y - \hat{f}_D(x))^2 \right] \\
&= \mathbb{E}_{D, \varepsilon} \left[ (f(x) + \varepsilon - \hat{f}_D(x))^2 \right]
\end{aligned}
\]

为了简化，我们引入一个记号：令 \( \hat{f} = \hat{f}_D(x) \)。关键在于，我们需要处理 \( \hat{f} \) 和 \( \varepsilon \) 的随机性。

推导的核心技巧是**在括号内同时加上和减去 \( \mathbb{E}_D[\hat{f}] \)**（即模型在所有可能训练集上的平均预测）：

\[
\begin{aligned}
\text{误差} &= \mathbb{E}_{D, \varepsilon} \left[ (f(x) + \varepsilon - \hat{f})^2 \right] \\
&= \mathbb{E}_{D, \varepsilon} \left[ \left( (f(x) - \mathbb{E}_D[\hat{f}]) + (\mathbb{E}_D[\hat{f}] - \hat{f}) + \varepsilon \right)^2 \right]
\end{aligned}
\]

现在，我们将这三项分别记为：
*   \( A = f(x) - \mathbb{E}_D[\hat{f}] \) （一个常数，与 \( D \) 和 \( \varepsilon \) 无关）
*   \( B = \mathbb{E}_D[\hat{f}] - \hat{f} \) （一个均值为0的随机变量，依赖于 \( D \)）
*   \( C = \varepsilon \) （一个均值为0的随机变量）

所以，误差 = \( \mathbb{E}[(A + B + C)^2] \)。展开这个平方项：

\[
\mathbb{E}[A^2 + B^2 + C^2 + 2AB + 2AC + 2BC]
\]

由于期望的线性性质，我们可以分别求每一项的期望。我们来逐一分析：

1.  **\( \mathbb{E}[A^2] \)**: \( A \) 是常数，所以 \( \mathbb{E}[A^2] = A^2 = (f(x) - \mathbb{E}_D[\hat{f}])^2 \)。这正是**偏差的平方**！
    *   \( \text{Bias}^2(\hat{f}(x)) = (f(x) - \mathbb{E}_D[\hat{f}])^2 \)

2.  **\( \mathbb{E}[B^2] \)**: \( B = \mathbb{E}_D[\hat{f}] - \hat{f} \)。注意 \( \mathbb{E}_D[B] = 0 \)。根据方差的定义 \( \text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] \)，所以 \( \mathbb{E}[B^2] \) 就是 \( \hat{f} \) 的**方差**！
    *   \( \text{Variance}(\hat{f}(x)) = \mathbb{E}_D[(\hat{f} - \mathbb{E}_D[\hat{f}])^2] \)

3.  **\( \mathbb{E}[C^2] \)**: \( C = \varepsilon \)。同样，\( \mathbb{E}[\varepsilon] = 0 \)，所以 \( \mathbb{E}[C^2] \) 就是 \( \varepsilon \) 的方差。
    *   \( \text{Noise} = \mathbb{E}[\varepsilon^2] = \sigma^2 \)

4.  **\( \mathbb{E}[2AB] \)**： \( 2A \) 是常数，可以提出来：\( 2A \cdot \mathbb{E}[B] \)。因为 \( \mathbb{E}[B] = 0 \)，所以这一项为 **0**。

5.  **\( \mathbb{E}[2AC] \)**： \( 2A \) 是常数，\( \mathbb{E}[C] = \mathbb{E}[\varepsilon] = 0 \)，所以这一项也为 **0**。

6.  **\( \mathbb{E}[2BC] \)**： \( B \) 依赖于训练集 \( D \)，\( C \) 是数据噪声。通常我们假设它们相互独立，所以 \( \mathbb{E}[B \cdot C] = \mathbb{E}[B] \cdot \mathbb{E}[C] = 0 \)。所以这一项也为 **0**。

---

### 3. 最终公式

将所有不为零的项组合起来，我们就得到了著名的**偏差-方差分解公式**：

\[
\boxed{
\mathbb{E}_{D, \varepsilon} \left[ (y - \hat{f}_D(x))^2 \right] = \underbrace{(f(x) - \mathbb{E}_D[\hat{f}_D(x)])^2}_{\text{偏差的平方}} + \underbrace{\mathbb{E}_D[(\hat{f}_D(x) - \mathbb{E}_D[\hat{f}_D(x)])^2]}_{\text{方差}} + \underbrace{\sigma^2}_{\text{不可消除的误差}}
}
\]

### 公式解读

*   **偏差的平方**： \( (\text{真实值} - \text{预测的平均值})^2 \)。衡量了模型的**系统性错误**。如果我们的模型在所有可能训练集上的平均预测都离真实值很远，说明模型本身有偏差。
*   **方差**： \( \mathbb{E}[(\text{单个模型预测} - \text{预测的平均值})^2] \)。衡量了模型的**不稳定性**。如果换一个训练集，模型的预测结果波动很大，说明方差很高。
*   **不可消除的误差**： \( \sigma^2 \)。这是数据本身固有的噪声，无论我们使用什么模型都无法避免。

这个优美的公式从数学上严格证明了我们之前的直观理解：**一个模型的泛化误差是由其偏差、方差和固有噪声三者共同构成的**。而我们建模的核心，就是通过调整模型复杂度，在偏差和方差之间进行权衡，以使这个总和最小化。
## 3 Methods
* Regularization: 添加正则化项(L1或L2)
* cross-validation: 
* ensemble methods: 

以下详细讲解这三种强大且常用的机器学习技术：**正则化**、**交叉验证**和**集成方法**。它们是构建稳健、高性能模型的三大支柱。

---

### 1. 正则化(Regularization)

**核心思想：** 通过在模型的经验损失函数上增加一个**惩罚项**，来限制模型的复杂度，从而防止过拟合。

**它解决了什么问题？**
- **过拟合**：模型在训练集上表现极好，但在未见过的测试集上表现很差。
- **高方差**：模型对训练数据中的噪声过于敏感。

**工作原理：**
原始的模型目标是最小化损失函数：`Minimize(Loss(预测值, 真实值))`
加入正则化后，目标变为：`Minimize(Loss(预测值, 真实值) + λ * Penalty(权重))`
这里的 `λ` 是正则化强度超参数，控制惩罚的力度。

**主要类型：**
- **L1正则化(Lasso)**：惩罚项是权重的绝对值之和。它倾向于产生**稀疏解**，即会将不重要的特征的权重直接压缩到0，因此自带**特征选择**功能。
- **L2正则化(Ridge)**：惩罚项是权重的平方和。它倾向于让所有权重都**均匀地变小**，但不会变为0，使得模型输出更加平滑。

**比喻：**
像一个严格的教练。L1教练会直接开除（权重归零）表现不佳的队员（特征）；而L2教练则要求所有队员都减重（权重缩小），但不开除任何人，让整个团队更灵活协调。

**何时使用？**
- 当你怀疑模型过于复杂、开始记忆噪声时。
- 当特征数量很多，需要自动进行特征选择时（用L1）。

---

### 2. 交叉验证(boosting)

**核心思想：** 一种评估模型泛化能力的统计方法，通过将数据**多次分割**为不同的训练集和验证集，来更可靠地估计模型在未知数据上的表现。

**它解决了什么问题？**
- **评估不可靠**：如果只进行一次训练集/测试集分割，评估结果可能因数据分割的随机性而有很大波动。
- **过拟合验证集**：在调参时，如果反复使用同一个测试集，模型可能会间接地"偷看"测试集信息，导致评估失真。

**工作原理（以最常用的k折交叉验证为例）：**
1.  将全部数据集**随机**划分为k个大小相似的子集（或称"折"）。
2.  进行k次循环，每次循环：
    - 选取**第i个折**作为**验证集**。
    - 将**剩余的k-1个折**合并作为**训练集**。
    - 在该训练集上训练模型，并在验证集上评估性能，得到一个分数。
3.  最终，将k次评估的分数**取平均值**，作为模型泛化能力的可靠估计。



**比喻：**
像一个多轮面试流程。一个候选人（模型）需要经过多个不同面试官小组（不同的验证集）的考核，最终取平均分来决定是否录用，这比只由一位面试官决定要公平和可靠得多。

**何时使用？**
- **模型评估**：当你想要一个对模型性能更稳健、偏差更小的估计时。
- **超参数调优**：与网格搜索结合，为模型选择最佳的超参数（如正则化强度λ、树的深度等）。

---

### 3. 集成方法(Ensemble)

**核心思想：** "三个臭皮匠，顶个诸葛亮"。通过构建并结合多个**基学习器**来完成学习任务，从而获得比单一学习器显著优越的泛化性能。

**它解决了什么问题？**
- **单一模型的不稳定性**：单个模型可能容易陷入局部最优或对数据敏感。
- **性能瓶颈**：单一模型的预测能力有限。

**主要类型与工作原理：**

1.  **Bagging - 并行集成**
    - **思想**：通过**自助采样法** 从原始数据集中产生多个不同的训练子集，然后并行地训练多个基学习器，最后通过**投票**或**平均**结合预测结果。
    - **目标**：主要**降低方差**。特别适用于那些复杂、容易过拟合的模型（如决策树）。
    - **代表算法**：**随机森林**。它是Bagging思想与决策树的完美结合，在构建每棵树时还引入了特征的随机选择，进一步增强了多样性。
    - 相对较快, 效果适中

2.  **Boosting - 序列集成**
    - **思想**：按**顺序**训练一系列基学习器。每个后续的模型都更加关注前序模型**预测错误**的样本，通过不断修正错误来提升性能。最后通过**加权投票**结合。
    - **目标**：主要**降低偏差**。将多个"弱"学习器提升为一个强大的"强"学习器。
    - **代表算法**：**AdaBoost, Gradient Boosting Machine, XGBoost, LightGBM**。这些是当今数据科学竞赛和工业界中最强大、最常用的算法之一。(这三个算法ppt中都有讲,底下给了附注)
    - 速度适中, 效果较好

3.  **Stacking - 分层集成**
    - **思想**：训练多个**不同类型**的基学习器，然后使用一个**元学习器**来学习如何最好地组合这些基学习器的预测结果。
    - **目标**：发挥不同模型的优势，实现"模型融合"。
    - **流程**：第一层模型对原始数据进行预测，其预测结果作为新的特征输入给第二层的元模型进行最终预测。

**比喻：**
- **Bagging**：像一个**专家评审团**。每个专家独立评审后，通过少数服从多数（或取平均）得出最终结论。
- **Boosting**：像一个**学生刷题**。学生先做一套题，然后重点复习做错的题目，再做下一套，如此反复，直到成绩优秀。
- **Stacking**：像一个**首席决策官**。他手下有市场、技术、财务等多个部门的负责人（第一层模型），他听取所有负责人的意见后，再做出最终的综合性决策。

**何时使用？**
- 当你希望最大限度地提升模型预测精度时。
- 当单一模型表现不稳定或遇到性能瓶颈时。

---

### 总结与关联

| 方法 | 核心目标 | 主要作用 | 典型应用场景 |
| :--- | :--- | :--- | :--- |
| **正则化** | 防止过拟合 | 控制模型复杂度，降低方差 | 线性模型、神经网络、任何容易过拟合的模型 |
| **交叉验证** | 可靠评估 & 调参 | 评估模型泛化能力，选择超参数 | 模型比较、超参数优化（与网格搜索结合） |
| **集成方法** | 提升预测性能 | 结合多个模型，降低方差/偏差 | 追求高精度的预测任务（如竞赛、推荐系统） |

**它们如何协同工作？**

在实际的机器学习流程中，这三者常常是紧密结合的：
1.  你决定使用一个**集成方法**（比如梯度提升树）。
2.  为了找到这个集成模型的最佳超参数（如树的数量、深度、学习率等），你会使用**交叉验证**来评估不同参数组合的表现。
3.  在集成模型的基学习器内部（比如单棵决策树），你可能会应用**正则化**（如限制树深度、设置叶节点最小样本数）来防止每棵树过拟合，从而让整个集成模型更健壮。

这三者共同构成了现代机器学习工程师工具箱中最基础、最强大的部分。

##附: Boosting的几种常用算法

### 核心思想：Boosting
在深入每个算法之前，首先要理解Boosting的核心思想：
- **目标**：将多个弱的、简单的模型（通常是决策树，但也可以是其他模型）组合成一个强大的模型。
- **方式**：**顺序地**训练弱模型。每一个后续的模型都专注于**纠正前一个模型所犯的错误**。
- **与Bagging（如随机森林）的区别**：Bagging是并行地训练多个模型，然后取平均或投票；而Boosting是顺序的、有针对性的改进。

---

### 1. AdaBoost (Adaptive Boosting)

AdaBoost是Boosting家族中最具开创性的算法之一。

#### 核心思想
1.  **赋予样本权重**：为训练数据集中的每一个样本初始化一个权重（初始时所有样本权重相等）。
2.  **顺序训练**：
    - **第一步**：用当前的样本权重训练第一个弱模型（例如一个很浅的决策树，称为“决策树桩”）。
    - **第二步**：根据这个弱模型的**表现**来调整样本的权重。**被错误分类的样本权重会增加**，而被正确分类的样本权重会减少。
    - **第三步**：训练下一个弱模型，这时它会更加关注那些**权重高（即上一个模型分错）** 的样本。
3.  **模型权重**：每个弱模型在最终集成模型中的“话语权”（权重）是不同的。**准确率越高的弱模型，其权重也越高**。
4.  **组合模型**：将所有弱模型根据其权重进行加权求和（分类问题）或加权平均（回归问题），得到最终的强模型。

#### 打个比方
就像学生备考。第一次模拟考试后，学生把做错的题目重点标记（增加权重）。第二次复习时，他更专注于这些错题。第二次模考后，他又标记出新的错题，并投入更多精力。最后，他把多次模考的经验（弱模型）综合起来，每场模考的信心（模型权重）不同，最终去参加大考（预测）。

#### 优点
- 概念简单，易于理解。
- 在实践中非常有效，尤其对于二分类问题。
- 不太容易过拟合（相对于单个强模型）。

#### 缺点
- 对噪声数据和异常值比较敏感。

---

### 2. GBM (Gradient Boosting Machine)

GBM可以看作是AdaBoost的泛化，它从另一个角度来解释和实现Boosting。

#### 核心思想
1.  **使用梯度下降**：GBM不再通过调整样本权重来让后续模型关注残差，而是将**整个Boosting过程视为一个在函数空间（模型空间）的优化问题**。
2.  **拟合残差**：
    - 首先训练一个弱模型（如决策树）。
    - 计算这个模型的**预测值与真实值之间的“残差”**（即负梯度）。
    - 下一个弱模型的目标不再是直接预测y，而是去**拟合这个残差（负梯度）**，从而“修正”前一个模型的错误。
3.  **累加模型**：将新模型的预测结果乘以一个**学习率**，然后累加到之前模型的预测结果上。学习率是一个很小的数（如0.1），用于控制每一步修正的幅度，防止过拟合。
4.  **重复**：不断重复步骤2和3，直到达到指定的弱模型数量或残差足够小。

#### 打个比方
就像一个高尔夫球手要进洞。
- 他先大力朝洞口方向打一杆（第一个弱模型）。
- 他走过去，观察球现在离洞口还有多远、在哪个方向（计算残差）。
- 他根据当前的位置，用小一点的力量和更精准的角度再打一杆（第二个弱模型，拟合残差）。
- 如此反复，直到球进洞。每次击球都是在修正上一次击球后留下的“残差”。

#### 优点
- 通常比AdaBoost有更高的预测精度。
- 灵活性高，可以使用不同的损失函数（如平方损失用于回归，对数损失用于分类）。

#### 缺点
- 训练过程是顺序的，难以并行化，训练可能较慢。
- 参数调优比AdaBoost更复杂。
- 如果不对弱学习器进行约束（如树深度），很容易过拟合。

---

### 3. XGBoost (eXtreme Gradient Boosting)

XGBoost是GBM的一个高效、灵活的优化实现，它几乎成为了数据科学竞赛的“大杀器”。

#### 核心思想
XGBoost的核心思想与GBM完全相同，但它通过一系列**工程和算法上的优化**，使其在速度和性能上远超传统的GBM。

#### 主要优化与特点
1.  **正则化**：
    - 在目标函数中除了损失函数，还** explicitly 加入了正则化项**（控制模型复杂度，如叶子节点的权重和树的深度）。
    - 这能有效防止过拟合，使得模型泛化能力更强。这是它与传统GBM的一个关键区别。

2.  **二阶泰勒展开**：
    - GBM只使用了一阶导数（梯度）来优化。
    - XGBoost使用了**损失函数的一阶和二阶导数**，这类似于牛顿法，能更准确地确定下降方向和步长，从而收敛更快。

3.  **并行处理**：
    - 虽然Boosting是顺序的，但**在构建每一棵树的过程中可以并行**。
    - XGBoost对特征进行预排序并存储为块结构，在分裂节点寻找最佳分割点时可以并行计算，大大加快了训练速度。

4.  **处理缺失值**：
    - XGBoost内置了处理缺失值的机制。它会自动学习在遇到缺失值时，应该将样本划分到左子树还是右子树。

5.  **灵活性**：
    - 支持自定义的损失函数和评估准则。
    - 支持交叉验证。

6.  **剪枝**：
    - 采用**后剪枝**策略，在建树时允许生长到最大深度，然后再从叶子节点反向剪枝，去除没有正收益的分裂。这比GBM常用的预剪枝（限制深度）更有效。

#### 优点
- **速度快**：得益于并行处理和算法优化。
- **性能好**：由于正则化和更精确的优化，通常能获得非常好的准确率。
- **功能强大**：支持各种任务、自定义目标函数，能处理缺失值等。

#### 缺点
- 参数更多，调优相对复杂。
- 相对于更简单的模型，可解释性仍然是一个挑战（尽管可以通过特征重要性来理解）。

---

### 总结与演进关系

| 特性 | AdaBoost | GBM | XGBoost |
| :--- | :--- | :--- | :--- |
| **核心** | 调整样本权重，关注错分样本 | 拟合负梯度（残差） | 拟合负梯度，并加入二阶导数和正则化 |
| **关注点** | 样本 | 残差 | 残差 + 模型复杂度 |
| **顺序性** | 是 | 是 | 是 |
| **并行能力** | 弱 | 弱 | **强**（节点分裂时） |
| **过拟合控制**| 通过调整权重 | 通过学习率/树深度 | **通过正则化项、学习率、剪枝** |
| **速度** | 较快 | 较慢 | **非常快** |
| **精度** | 不错 | 很好 | **极佳** |
| **使用难度** | 简单 | 中等 | 较复杂（参数多） |

**演进关系**：
可以看作是一个不断优化和发展的过程：**AdaBoost** 提出了顺序纠正错误的思想 -> **GBM** 用梯度下降的框架统一并泛化了这种思想，提升了能力 -> **XGBoost** 在GBM的基础上，通过工程和算法的极致优化，实现了性能和速度的巨大飞跃。

后来还有 **LightGBM** 和 **CatBoost** 等，它们与XGBoost并称为现代GBM的“三巨头”，各自在不同的方面（如速度、类别特征处理）做了进一步的优化。

xd:Random forest和XGBoost是机器学习里面两个最强大的算法