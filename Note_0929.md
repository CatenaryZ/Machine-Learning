**第1页**  
Lecture 4.1: 分类：线性决策边界  
谢丹  
清华大学数学系  
2025年10月12日  

**第2页**  
分类问题  
目标：将输入数据 \( x \) 分配到 \( K \) 个类别中的某一个  
输入：特征向量 \( x \in \mathbb{R}^D \)  
输出：类别标签 \( y \in \{1, 2, \ldots, K\} \)  
方法：  
判别函数  
生成模型  
判别模型（例如，逻辑回归 Logistic Regression）  

**第3页**  
判别函数  
定义：为每个类别 \( k \) 定义一个函数 \( f_k(x) \)，直接将输入 \( x \) 映射到类别分配：  
\[y = \arg\max_{k} f_k(x)\]  
线性判别函数：  
\[f_k(x) = w_k^T x + b_k\]  
非线性判别函数：  
\[f_k(x) = \phi(w_k^T x + b_k)\]  
关键特性：直接对决策边界建模，无需估计概率分布  

**第4页**  
线性判别函数表述 I  
基本形式：  
\[ f(x) = w^T x + b \]  
其中：  
- \( w \)：权重向量  
- \( b \)：偏置项  
- \( x \)：输入特征向量  
分类规则：  
\[ y = \begin{cases} +1 & \text{如果 } f(x) \geq 0 \\ -1 & \text{如果 } f(x) < 0 \end{cases} \]  

**第5页**  
线性判别函数表述 II  
多类别扩展：  
\[ f_k(x) = w_k^T x + b_k, \quad y = \arg\max_k f_k(x) \]  
即，类别分配由函数值最大的那个决定。  

**第6页**  
（空白页）  

**第7页**  
方法 1: 最小二乘法 I  
目标函数：最小化平方误差和：  
\[J(w) = \sum_{i=1}^{N} (y_i - (w^T x_i + b))^2\]  
矩阵形式：  
\[J(w) = (y - Xw)^T (y - Xw)\]  
闭式解：  

**第8页**  
方法 1: 最小二乘法 II  
\[ w = (X^T X)^{-1} X^T y \]  
- 要求 \( X^T X \) 可逆  
- 对异常值敏感  
- 对于小数据集计算高效  
也可以通过添加正则化项 \(\lambda \sum w_i^2\) 来考虑误差函数（岭分类 Ridge Classification）。  

**第9页**  
方法 2: 感知器损失函数 I  
感知器学习背后的驱动力  
定义：感知器使用合页损失函数，定义为：  
感知器损失：  
\[L(w) = \sum_{i \in M} -y_i(w \cdot x_i + b)\]  
其中：  
- \( M \)：被错误分类的样本集合（预测与真实观测不符）  
- \( y_i \in \{-1,+1\} \)：真实标签  
- \( w \)：权重向量  
- \( b \)：偏置项  
- \( x_i \)：输入特征  

**第10页**  
方法 2: 感知器损失函数 II  
感知器学习背后的驱动力  
关键特性 & 梯度  
- **凸性**：保证收敛  
  \[  \nabla L(\mathbf{w}) = \sum_{i \in M} -y_i \mathbf{x}_i\]  
- **分段线性**：梯度简单  
  \[  \frac{\partial L}{\partial b} = \sum_{i \in M} -y_i\]  
- **对正确分类为零**  
  导致更新规则：  
- **对错误分类为正**  
  \[  \mathbf{w} \leftarrow \mathbf{w} + \eta y_i \mathbf{x}_i\]  

**第11页**  
方法 3: 支持向量机  
y_i = +1  
+ 男生  
○ 女生  
支持向量  
w^T x + b = 1  
最优超平面  
w^T x + b = 0  
身高  

**第12页**  
将支持向量机转化为规划问题 I  
从几何直观到优化表述  
原始几何问题：最大化间隔：\(\max \frac{2}{\|w\|}\)  
约束条件：对所有 \(i\)，\(y_i(w \cdot x_i + b) \geq 1\)（线性可分）  
步骤 1：等价重构：不最大化 \(\frac{2}{\|w\|}\)，而是最小化 \(\|w\|\)：  
\[\min \|w\| \quad \text{满足约束} \quad y_i(w \cdot w_i + b) \geq 1\]  
步骤 2：凸优化形式  

**第13页**  
将支持向量机转化为规划问题 II  
从几何直观到优化表述  
为计算方便，使用平方范数：  
\[\min \frac{1}{2} \|w\|^2 \quad \text{满足约束} \quad y_i(w \cdot x_i + b) \geq 1\]  
- [x] 凸目标函数  
- [x] 线性约束  
- [x] 二次规划问题  
步骤 3：原始二次规划形式  
性质  
\[\min_{w,b} \frac{1}{2} w^T w\]  
\[约束条件:\]  
\[y_i(w^T x_i + b) \geq 1, \quad i = 1, \ldots, n\]  
- [x] 凸目标  
- [x] 线性约束  
- [x] 保证全局最优  

**第14页**  
将支持向量机转化为规划问题 III  
从几何直观到优化表述  
步骤 4：实际实现  
- 使用二次规划求解器  
- 或使用专门的SVM库  
- 使用优化技术处理大型数据集  

**第15页**  
软间隔支持向量机 1  
软间隔支持向量机：处理噪声和重叠  
真实数据很少完美可分。我们引入松弛变量 \(\xi_i\) 以允许错误分类。  
\[\min_{w, b, \xi} \frac{1}{2} \| w \|^2 + C \sum_{i=1}^n \xi_i\]  
约束条件：  
\[y_i (w^T x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0 \quad \forall i\]  
参数 \(C\) 控制大间隔和正确分类点之间的权衡。  

**第16页**  
硬间隔支持向量机：完美分离  
理想情况  
假设线性可分数据，标签 \( y_i \in \{-1, +1\} \)  
- 决策边界：  
  \[  w^T x + b = 0\]  
- 间隔边界：  
  \[  w^T x + b = \pm 1\]  
- 约束条件：  
  \[  y_i(w^T x_i + b) \geq 1\]  

**第17页**  
软间隔支持向量机：处理现实  
硬间隔的问题  
真实数据很少完美可分！  
**解决方案：** 引入松弛变量 \(\xi_i \geq 0\)  
松弛约束  
\[y_i (w^T x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0\]  
松弛变量解释  
- \(\xi_i = 0\)：在间隔外正确分类  
- \(0 < \xi_i \leq 1\)：在间隔内但在正确的一侧  
- \(\xi_i > 1\)：错误分类  

**第18页**  
软间隔支持向量机优化  
原始问题  
\[ \min_{w, b, \xi} \frac{1}{2} \| w \|^2 + C \sum_{i=1}^n \xi_i \]  
约束条件：  
\[ y_i (w^T x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0 \]  
► \(\frac{1}{2} \| w \|^2\)：最大化间隔  
► \(\sum \xi_i\)：最小化分类错误  
► \(C > 0\)：权衡参数  

**第19页**  
关键观察  
约束告诉我们一些信息  
根据约束：  
\[\xi_i \geq 1 - y_i(w^T x_i + b)\]  
和  
\[\xi_i \geq 0\]  
优化洞察  
由于我们在最小化 \(\sum \xi_i\)，在最优解处：  
\[\xi_i = \max (0, 1 - y_i(w^T x_i + b))\]  
最优松弛由点的间隔违反程度决定！  

**第20页**  
代入与合页损失出现 I  
代入最优松弛  
将 \(\xi_i\) 代入目标函数：  
\[\min_{w,b} \frac{1}{2} \| w \|^2 + C \sum_{i=1}^n \max \big( 0, 1 - y_i (w^T x_i + b) \big)\]  
定义合页损失  
令 \( f(x_i) = w^T x_i + b \)，则：  
\[L_{\text{hinge}}(y_i, f(x_i)) = \max(0, 1 - y_i f(x_i))\]  
最终形式  

**第21页**  
代入与合页损失出现 II  
SVM 损失：  
\[\min_{w,b} \lambda \| w \|^2 + \frac{1}{n} \sum_{i=1}^n \max(0, 1 - y_i f(x_i))\]  
其中 \(\lambda = \frac{1}{2nC}\)  
因此，软间隔SVM可以被视为使用合页损失加上一个正则化项，并因此具有概率解释。  

**第22页**  
多类别训练策略  
一对多  
训练 K 个二元分类器 fk(x)。  
每个分类器将一个类别与所有其他类别分开（给定一个类别，将其他类别视为另一个类别）。  
最终：arg maxk fk(x)  
一对一  
► 训练 \(\frac{K(K-1)}{2}\) 个分类器 \(f_{ij}(x)\)。  
► 每个分类器区分一对类别  
► 最终：多数投票  

**第23页**  
生成模型  
贝叶斯方法  
使用以下公式对联合分布 \( p(x, y) \) 建模：  
\[p(y|x) = \frac{p(x|y)p(y)}{p(x)}\]  
- **类先验**：\( p(y) \) - 每个类别的概率  
- **类条件密度**：\( p(x|y) \) - 给定类别下特征的分布  
- **后验概率**：\( p(y|x) \) - 给定特征下类别的概率  
例子  
- 线性判别分析  
- 二次判别分析  
- 朴素贝叶斯分类器  

**第24页**  
线性判别分析  
► 类别：\( k = 1, 2, \ldots, K \)  
► 数据：\(\{(x_1, y_1), \ldots, (x_N, y_N)\}\)，其中 \( x_i \in \mathbb{R}^p\)  
► 类别标签：\( y_i \in \{1, 2, \ldots, K\}\)  
► 目标：使用最大似然估计估计LDA模型的参数  

**第25页**  
LDA 模型假设  
关键假设  
1. **类条件分布是高斯分布：**  
   \[P(x|y = k) = \mathcal{N}(x|\mu_k, \Sigma)\]  
2. **共享协方差矩阵：** 所有类别具有相同的 \(\Sigma\)  
3. **类先验：**  
   \[   P(y = k) = \pi_k,\]  
   且  
   \[\sum_{k=1}^K \pi_k = 1\]  
待估计参数  
- [ ] 类别均值：\(\mu_1, \ldots, \mu_K\)  
- [x] 公共协方差：\(\Sigma\)  
- [ ] 类先验：\(\pi_1, \ldots, \pi_K\)  

**第26页**  
完整数据似然  
联合概率  
\[P(x, y) = P(x|y)P(y)\]  
完整数据似然  
\[\mathcal{L}(\mu, \Sigma, \pi) = \prod_{i=1}^{N} P(x_i|y_i)P(y_i)\]  
令 \( C_k = \{i : y_i = k\} \) 且 \( N_k = |C_k| \)，则：  
\[\mathcal{L} = \prod_{k=1}^{K} \prod_{i \in C_k} \pi_k \cdot \mathcal{N}(x_i|\mu_k, \Sigma)\]  

**第27页**  
对数似然函数  
取对数  
\[\ell = \log L = \sum_{k=1}^{K} \sum_{i \in C_k} [\log \pi_k + \log N(x_i | \mu_k, \Sigma)]\]  
高斯对数密度  
\[\log N(x | \mu, \Sigma) = -\frac{p}{2} \log(2\pi) - \frac{1}{2} \log |\Sigma| - \frac{1}{2}(x - \mu)^T \Sigma^{-1}(x - \mu)\]  
完整对数似然  
\[\ell = \sum_{k=1}^{K} N_k \log \pi_k - \frac{Np}{2} \log(2\pi) - \frac{N}{2} \log |\Sigma| - \frac{1}{2} \sum_{k=1}^{K} \sum_{i \in C_k} (x_i - \mu_k)^T \Sigma^{-1}(x_i - \mu_k)\]  

**第28页**  
类先验 \(\pi_k\) 的最大似然估计  
约束优化  
在 \(\sum_{k=1}^K \pi_k = 1\) 约束下最大化 \(\sum_{k=1}^K N_k \log \pi_k\) 拉格朗日函数：  
\[\mathcal{L}_{\pi} = \sum_{k=1}^K N_k \log \pi_k + \lambda \left( 1 - \sum_{k=1}^K \pi_k \right)\]  
一阶条件  
\[\frac{\partial \mathcal{L}_{\pi}}{\partial \pi_k} = \frac{N_k}{\pi_k} - \lambda = 0 \quad \Rightarrow \quad \pi_k = \frac{N_k}{\lambda}\]  
\[\sum_{k=1}^K \pi_k = \sum_{k=1}^K \frac{N_k}{\lambda} = \frac{N}{\lambda} = 1 \quad \Rightarrow \quad \lambda = N\]  

**第29页**  
类先验 \(\pi_k\) 的最大似然估计 II  
MLE 解  
\[\hat{\pi}_k = \frac{N_k}{N}\]  

**第30页**  
类别均值 \(\mu_k\) 的最大似然估计  
对数似然的相关部分  
\[\ell_\mu = -\frac{1}{2} \sum_{k=1}^K \sum_{i \in C_k} (x_i - \mu_k)^T \Sigma^{-1} (x_i - \mu_k)\]  
求导  
\[\frac{\partial \ell_\mu}{\partial \mu_k} = \sum_{i \in C_k} \Sigma^{-1} (x_i - \mu_k) = 0\]  
由于 \(\Sigma^{-1}\) 可逆：  
\[\sum_{i \in C_k} (x_i - \mu_k) = 0 \quad \Rightarrow \quad N_k \mu_k = \sum_{i \in C_k} x_i\]  

**第31页**  
类别均值 \(\mu_k\) 的最大似然估计 II  
MLE 解  
\[\hat{\mu}_k = \frac{1}{N_k} \sum_{i \in C_k} x_i\]  

**第32页**  
公共协方差 \(\Sigma\) 的最大似然估计  
对数似然的相关部分  
\[\ell_{\Sigma} = -\frac{N}{2} \log |\Sigma| - \frac{1}{2} \sum_{k=1}^{K} \sum_{i \in C_k} (x_i - \mu_k)^T \Sigma^{-1}(x_i - \mu_k)\]  
矩阵迹恒等式  
使用 \(a^T Ba = \operatorname{tr}(Baa^T)\)：  
\[\sum_{k=1}^{K} \sum_{i \in C_k} (x_i - \mu_k)^T \Sigma^{-1}(x_i - \mu_k) = \operatorname{tr}(\Sigma^{-1} S)\]  
其中 \(S = \sum_{k=1}^{K} \sum_{i \in C_k} (x_i - \mu_k)(x_i - \mu_k)^T\)  

**第33页**  
求解 \(\Sigma\) I  
简化目标  
\[\ell_{\sum} = -\frac{N}{2} \log |\Sigma| - \frac{1}{2} \operatorname{tr}(\Sigma^{-1}S)\]  
矩阵导数  
\[\frac{\partial \log |\Sigma|}{\partial \Sigma} = \Sigma^{-1}\]  
\[\frac{\partial \operatorname{tr}(\Sigma^{-1}S)}{\partial \Sigma} = -\Sigma^{-1}S\Sigma^{-1}\]  
一阶条件  

**第34页**  
求解 \(\Sigma\) II  
\[\frac{\partial \ell_{\sum}}{\partial \sum} = -\frac{N}{2} \sum^{-1} + \frac{1}{2} \sum^{-1} S \sum^{-1} = 0\]  
左右乘以 \(2\sum\)：  
\[-N \sum + S = 0 \quad \Rightarrow \quad \sum = \frac{1}{N} S\]  

**第35页**  
最终 LDA MLE 估计量  
完整的 MLE 估计量集合  
1. 类先验：\( \hat{\pi}_k = \frac{N_k}{N} \)  
2. 类别均值：\( \hat{\mu}_k = \frac{1}{N_k} \sum_{i \in C_k} x_i \)  
3. 公共协方差：  
   \[ \hat{\Sigma} = \frac{1}{N} \sum_{k=1}^K \sum_{i \in C_k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T \]  
无偏版本（常用实践）：  
   \[ \hat{\Sigma}_{\text{unbiased}} = \frac{1}{N-K} \sum_{k=1}^K \sum_{i \in C_k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T \]  

**第36页**  
LDA 分类规则  
使用估计参数的贝叶斯分类器  
\[\hat{y} = \arg \max_k \hat{\pi}_k \cdot \mathcal{N}(\mathbf{x}|\hat{\mu}_k, \hat{\Sigma})\]  
为什么是"线性"的？  
由于共享 \(\Sigma\)，判别函数 \(\delta_k(\mathbf{x})\) 在 \(\mathbf{x}\) 中是线性的：  
\[\delta_k(\mathbf{x}) = \mathbf{x}^T \hat{\Sigma}^{-1} \hat{\mu}_k - \frac{1}{2} \hat{\mu}_k^T \hat{\Sigma}^{-1} \hat{\mu}_k + \log \hat{\pi}_k\]  

**第37页**  
二次判别分析  
假设  
- **高斯类条件密度**  
- **类别特定的协方差矩阵**  
- \( p(x|y = k) = \mathcal{N}(x|\mu_k, \Sigma_k) \)  
\[\log p(y = k|x) \propto -\frac{1}{2}(x - \mu_k)^T\Sigma_k^{-1}(x - \mu_k) - \frac{1}{2}\log|\Sigma_k| + \log\pi_k\]  
决策边界  
**由于不同的协方差矩阵，在 x 中是二次的：**  
\[(x - \mu_1)^T\Sigma_1^{-1}(x - \mu_1) + \log|\Sigma_1| = (x - \mu_2)^T\Sigma_2^{-1}(x - \mu_2) + \log|\Sigma_2|\]  

**第38页**  
什么是朴素贝叶斯？  
定义  
朴素贝叶斯是一种基于贝叶斯定理的概率分类算法，其假设特征之间在给定类别标签的情况下是强（朴素）独立的。  
- **简单**而强大  
- **快速**训练和预测  
- **概率**输出  
- 在高维数据上表现良好  

**第39页**  
为什么"朴素"？  
朴素假设  
在给定类别标签的条件下，特征之间条件独立：  
\[P(X_1, X_2, \ldots, X_d | Y) = P(X_1 | Y) \cdot P(X_2 | Y) \cdots P(X_d | Y)\]  
现实世界例子  
- 将电子邮件分类为垃圾邮件/非垃圾邮件  
- 像"免费"和"金钱"这样的词可能是相关的  
- 朴素贝叶斯假设在给定"垃圾邮件"的条件下它们是独立的  
- 令人惊讶的是，这在实践中通常效果很好！  

**第40页**  
朴素贝叶斯概率模型  
完整公式  
对于特征 \( X_1, X_2, \ldots, X_d \) 和类别 \( Y \)：  
\[P(Y|X_1, \ldots, X_d) = \frac{P(Y) \prod_{j=1}^d P(X_j|Y)}{P(X_1, \ldots, X_d)}\]  
分类规则  
我们预测概率最高的类别：  
\[\hat{y} = \arg \max_y P(y) \prod_{j=1}^d P(x_j|y)\]  
- \( P(X_1, \ldots, X_d) \) 对所有类别是常数  
- 在比较时我们可以忽略它  

**第41页**  
高斯朴素贝叶斯  
对于连续特征  
假设特征服从正态分布：  
\[P(X_j | Y = y_k) = \frac{1}{\sqrt{2\pi\sigma_{jk}^2}} \exp\left(-\frac{(x_j - \mu_{jk})^2}{2\sigma_{jk}^2}\right)\]  
参数估计  
- \(\mu_{jk} = \frac{1}{n_k} \sum_{i:y_i = y_k} x_j^{(i)}\)  
- \(\sigma_{jk}^2 = \frac{1}{n_k} \sum_{i:y_i = y_k} (x_j^{(i)} - \mu_{jk})^2\)  
- \(n_k\)：类别 \(y_k\) 中的样本数  

**第42页**  
多项朴素贝叶斯  
对于离散计数  
常用于文本分类：  
\[P(X_j|Y = y_k) = \frac{\text{count}(X_j, Y = y_k) + \alpha}{\sum_{l=1}^d \text{count}(X_l, Y = y_k) + \alpha d}\]  
- \(\alpha\)：平滑参数  
- \(\alpha\)：防止零概率  
- 当 \(\alpha = 1\) 时为拉普拉斯平滑  
例子  
文档中的词频：______  
单词 垃圾邮件计数 免费 150 金钱 120 ...  

**第43页**  
伯努利朴素贝叶斯  
对于二元特征  
模拟特征的存在/缺失：  
\[P(X_j|Y = y_k) = P(j|y_k)^{x_j}(1 - P(j|y_k))^{1-x_j}\]  
应用  
- \( x_j = 1 \) 如果特征 \( j \) 存在  
- \( x_j = 0 \) 如果特征 \( j \) 缺失  
- 对于使用二元词存在性的文档分类有用  

**第44页**  
训练算法  
步骤 1：估计先验概率  
\[P(Y = y_k) = \frac{\text{类别 } y_k \text{ 的样本数}}{\text{总样本数}}\]  
步骤 2：估计似然  
- **高斯**：计算每个类别下每个特征的均值和方差  
- **多项**：计算每个类别下每个特征的频率计数  
- **伯努利**：计算每个类别下特征存在的概率  

**第45页**  
判别模型：逻辑回归  
判别方法  
直接对后验概率 \( p(y|x) \) 建模，而不对 \( p(x|y) \) 建模  
二分类情况：  
\[p(y = 1|x) = \sigma(w^T x + b)\]  
\[\sigma(z) = \frac{1}{1 + e^{-z}}\]  
多分类情况（Softmax）：  
\[p(y = k|x) = \frac{\exp(w_k^T x + b_k)}{\sum_{j=1}^K \exp(w_j^T x + b_j)}\]  
优点  
与生成模型相比，对数据分布的假设更少  

**第46页**  
最大似然估计  
最大化训练数据的对数似然：  
\[\mathcal{L}(w) = \sum_{i=1}^{N} [y_i \log p(y_i|x_i, w) + (1 - y_i) \log(1 - p(y_i|x_i, w))]\]  
二分类情况的梯度  
\[\nabla_w \mathcal{L} = \sum_{i=1}^{N} (y_i - p(y_i = 1|x_i, w)) x_i\]  

**第47页**  
方法比较  
| 方法 | 类型 | 边界 | 假设 | 优点 |
|---|---|---|---|---|
| 判别函数 | 非概率 | 灵活 | 无 | 简单，快速 |
| LDA | 生成式 | 线性 | 高斯，共享协方差 | 对小数据稳健 |
| QDA | 生成式 | 二次 | 高斯，不同协方差 | 灵活边界 |
| 逻辑回归 | 判别式 | 线性 | 线性决策边界 | 对分类最优 |

**表格：分类方法比较**

- **生成式**：对小数据集更好，可以生成样本，处理缺失数据
- **判别式**：通常在大数据集上性能更好，专注于决策边界

**第48页**  
何时使用每种方法 I  
判别函数  
- **当不需要概率解释时**  
- **当计算效率至关重要时**  
- **用于简单、可解释的模型**  
生成模型  
- **当数据集较小时**  
- **当你想生成新样本时**  
- **当特征近似服从高斯分布时**  
- **当你需要处理缺失数据时**  
逻辑回归  

**第49页**  
何时使用每种方法 II  
► 用于大型数据集  
► 当你需要良好校准的概率时  
► 当高斯假设被违反时  
► 作为更复杂模型的基线  

**第50页**  
非参数方法  
K最近邻，决策树，随机森林。